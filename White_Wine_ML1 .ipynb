{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "White_Wine_ML1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhwv0JVk4OYf",
        "outputId": "7b9e763f-cff3-4571-fe63-e1553508ba23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.0.0'\n",
        "spark_version = 'spark-3.0.1'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "!pip install psycopg2-binary\n",
        "!pip install keras-tuner\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "# Import our dependencies\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sqlalchemy import create_engine\n",
        "from kerastuner.tuners import RandomSearch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.39)] [Co\r                                                                               \rGet:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [2 InRelease 47.5 kB/88.7 kB 54%] [Connecting to security.ubuntu.com (91.189\r                                                                               \rGet:4 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "\r0% [2 InRelease 50.4 kB/88.7 kB 57%] [Connecting to security.ubuntu.com (91.189\r0% [1 InRelease gpgv 242 kB] [2 InRelease 50.4 kB/88.7 kB 57%] [Connecting to s\r0% [1 InRelease gpgv 242 kB] [Connecting to security.ubuntu.com (91.189.91.39)]\r0% [1 InRelease gpgv 242 kB] [Connecting to security.ubuntu.com (91.189.91.39)]\r                                                                               \rGet:5 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:13 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,687 kB]\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [864 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,167 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,119 kB]\n",
            "Get:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [48.9 kB]\n",
            "Get:18 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [40.3 kB]\n",
            "Ign:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:20 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [405 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,354 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,750 kB]\n",
            "Fetched 10.7 MB in 3s (3,511 kB/s)\n",
            "Reading package lists... Done\n",
            "Collecting psycopg2-binary\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/1b/720b36697158113ca1b2221a8e96a470088ccf3770d182214689d1a96a07/psycopg2_binary-2.8.6-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 7.2MB/s \n",
            "\u001b[?25hInstalling collected packages: psycopg2-binary\n",
            "Successfully installed psycopg2-binary-2.8.6\n",
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/f7/4b41b6832abf4c9bef71a664dc563adb25afc5812831667c6db572b1a261/keras-tuner-1.0.1.tar.gz (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.18.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.6.20)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (0.17.0)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-cp36-none-any.whl size=73200 sha256=9aafaae99ce08fae337f80a64c2825f22994fe4ac79593a24472f71098aba794\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/cc/62/52716b70dd90f3db12519233c3a93a5360bc672da1a10ded43\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=9e0c392edc8722367dac17a6833f560db5b9fab542fd7f55a475979d215ef9c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.1 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lA2YiP5p4diM",
        "outputId": "b3526dbd-5014-465d-b0d4-626f4c921aae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# enter the following code to download a Postgres driver that will allow Spark to interact with Postgres:\n",
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.16.jar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-11-07 18:42:51--  https://jdbc.postgresql.org/download/postgresql-42.2.16.jar\n",
            "Resolving jdbc.postgresql.org (jdbc.postgresql.org)... 72.32.157.228, 2001:4800:3e1:1::228\n",
            "Connecting to jdbc.postgresql.org (jdbc.postgresql.org)|72.32.157.228|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1002883 (979K) [application/java-archive]\n",
            "Saving to: ‘postgresql-42.2.16.jar’\n",
            "\n",
            "postgresql-42.2.16. 100%[===================>] 979.38K  1.36MB/s    in 0.7s    \n",
            "\n",
            "2020-11-07 18:42:53 (1.36 MB/s) - ‘postgresql-42.2.16.jar’ saved [1002883/1002883]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7Dpxt-P4eci"
      },
      "source": [
        "# start a Spark session with an additional option that adds the driver to Spark:\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Wine_Weather\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.16.jar\").getOrCreate()\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufHy0SIvYK7W"
      },
      "source": [
        "##***White Wine Machine Learning Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5cDkhQuDASb",
        "outputId": "69b4f333-82f0-42a9-f5c6-ba81ebd25b3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "source": [
        "#Read white wine sql table into a dataframe\n",
        "White_Soil_ML_df = pd.read_sql_table('white_soil_table', 'postgresql://postgres:postgres@database-1.cslpjur96f9r.us-east-2.rds.amazonaws.com:5432') \n",
        "White_Soil_ML_df.head() "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>appellation</th>\n",
              "      <th>wine</th>\n",
              "      <th>wine_id</th>\n",
              "      <th>color</th>\n",
              "      <th>regions</th>\n",
              "      <th>country</th>\n",
              "      <th>vintage</th>\n",
              "      <th>is_primeurs</th>\n",
              "      <th>score</th>\n",
              "      <th>confidence_index</th>\n",
              "      <th>journalist_count</th>\n",
              "      <th>avgPrcpFebruary</th>\n",
              "      <th>avgTempFebruary</th>\n",
              "      <th>avgPrcpMarch</th>\n",
              "      <th>avgTempMarch</th>\n",
              "      <th>avgPrcpApril</th>\n",
              "      <th>avgTempApril</th>\n",
              "      <th>avgPrcpMay</th>\n",
              "      <th>avgTempMay</th>\n",
              "      <th>avgPrcpJune</th>\n",
              "      <th>avgTempJune</th>\n",
              "      <th>avgPrcpJuly</th>\n",
              "      <th>avgTempJuly</th>\n",
              "      <th>avgPrcpAugust</th>\n",
              "      <th>avgTempAugust</th>\n",
              "      <th>avgPrcpSeptember</th>\n",
              "      <th>avgTempSeptember</th>\n",
              "      <th>avgPrcpOctober</th>\n",
              "      <th>avgTempOctober</th>\n",
              "      <th>bdod_0-100cm</th>\n",
              "      <th>bdod_100-200cm</th>\n",
              "      <th>cec_0-100cm</th>\n",
              "      <th>cec_100-200cm</th>\n",
              "      <th>cfvo_0-100cm</th>\n",
              "      <th>cfvo_100-200cm</th>\n",
              "      <th>clay_0-100cm</th>\n",
              "      <th>clay_100-200cm</th>\n",
              "      <th>nitrogen_0-100cm</th>\n",
              "      <th>nitrogen_100-200cm</th>\n",
              "      <th>ocd_0-100cm</th>\n",
              "      <th>ocd_100-200cm</th>\n",
              "      <th>ocs_0-30cm</th>\n",
              "      <th>phh2o_0-100cm</th>\n",
              "      <th>phh2o_100-200cm</th>\n",
              "      <th>sand_0-100cm</th>\n",
              "      <th>sand_100-200cm</th>\n",
              "      <th>silt_0-100cm</th>\n",
              "      <th>silt_100-200cm</th>\n",
              "      <th>soc_0-100cm</th>\n",
              "      <th>soc_100-200cm</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Santa Cruz Mountains</td>\n",
              "      <td>Mount Eden Vineyards, Chardonnay, White, Santa...</td>\n",
              "      <td>107658</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2015</td>\n",
              "      <td>False</td>\n",
              "      <td>92.22</td>\n",
              "      <td>B</td>\n",
              "      <td>4</td>\n",
              "      <td>0.174747</td>\n",
              "      <td>58</td>\n",
              "      <td>0.018630</td>\n",
              "      <td>60</td>\n",
              "      <td>0.096254</td>\n",
              "      <td>59</td>\n",
              "      <td>0.002989</td>\n",
              "      <td>60</td>\n",
              "      <td>0.000645</td>\n",
              "      <td>65</td>\n",
              "      <td>0.001132</td>\n",
              "      <td>69</td>\n",
              "      <td>0.000947</td>\n",
              "      <td>70</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>68</td>\n",
              "      <td>0.005581</td>\n",
              "      <td>66</td>\n",
              "      <td>139.75</td>\n",
              "      <td>149</td>\n",
              "      <td>153.4</td>\n",
              "      <td>145</td>\n",
              "      <td>183.5</td>\n",
              "      <td>245</td>\n",
              "      <td>197.50</td>\n",
              "      <td>193</td>\n",
              "      <td>145.7</td>\n",
              "      <td>60</td>\n",
              "      <td>124.95</td>\n",
              "      <td>25</td>\n",
              "      <td>60</td>\n",
              "      <td>5.50206</td>\n",
              "      <td>5.9</td>\n",
              "      <td>442.10</td>\n",
              "      <td>468</td>\n",
              "      <td>324.85</td>\n",
              "      <td>283</td>\n",
              "      <td>128.55</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Napa Valley</td>\n",
              "      <td>Pahlmeyer, Napa Valley Chardonnay, White, Napa...</td>\n",
              "      <td>111897</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2015</td>\n",
              "      <td>False</td>\n",
              "      <td>92.83</td>\n",
              "      <td>C+</td>\n",
              "      <td>4</td>\n",
              "      <td>0.176970</td>\n",
              "      <td>56</td>\n",
              "      <td>0.003653</td>\n",
              "      <td>57</td>\n",
              "      <td>0.101661</td>\n",
              "      <td>57</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60</td>\n",
              "      <td>0.010968</td>\n",
              "      <td>65</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>69</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69</td>\n",
              "      <td>0.007210</td>\n",
              "      <td>68</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>65</td>\n",
              "      <td>140.80</td>\n",
              "      <td>150</td>\n",
              "      <td>290.2</td>\n",
              "      <td>289</td>\n",
              "      <td>153.5</td>\n",
              "      <td>150</td>\n",
              "      <td>282.55</td>\n",
              "      <td>273</td>\n",
              "      <td>72.6</td>\n",
              "      <td>40</td>\n",
              "      <td>109.20</td>\n",
              "      <td>31</td>\n",
              "      <td>45</td>\n",
              "      <td>5.90206</td>\n",
              "      <td>6.4</td>\n",
              "      <td>203.00</td>\n",
              "      <td>191</td>\n",
              "      <td>418.85</td>\n",
              "      <td>410</td>\n",
              "      <td>84.00</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sonoma Coast</td>\n",
              "      <td>Marcassin, Lorenzo Vineyard Chardonnay, White,...</td>\n",
              "      <td>101640</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>1993</td>\n",
              "      <td>False</td>\n",
              "      <td>92.07</td>\n",
              "      <td>C</td>\n",
              "      <td>3</td>\n",
              "      <td>0.275357</td>\n",
              "      <td>58</td>\n",
              "      <td>0.086129</td>\n",
              "      <td>67</td>\n",
              "      <td>0.050667</td>\n",
              "      <td>68</td>\n",
              "      <td>0.068929</td>\n",
              "      <td>73</td>\n",
              "      <td>0.029333</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>85</td>\n",
              "      <td>0.058710</td>\n",
              "      <td>77</td>\n",
              "      <td>145.70</td>\n",
              "      <td>156</td>\n",
              "      <td>180.8</td>\n",
              "      <td>198</td>\n",
              "      <td>39.5</td>\n",
              "      <td>55</td>\n",
              "      <td>200.65</td>\n",
              "      <td>205</td>\n",
              "      <td>77.5</td>\n",
              "      <td>36</td>\n",
              "      <td>121.65</td>\n",
              "      <td>24</td>\n",
              "      <td>52</td>\n",
              "      <td>5.40206</td>\n",
              "      <td>5.8</td>\n",
              "      <td>441.75</td>\n",
              "      <td>477</td>\n",
              "      <td>319.20</td>\n",
              "      <td>251</td>\n",
              "      <td>98.60</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sonoma Coast</td>\n",
              "      <td>Marcassin, Lorenzo Vineyard Chardonnay, White,...</td>\n",
              "      <td>101640</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>1998</td>\n",
              "      <td>False</td>\n",
              "      <td>91.74</td>\n",
              "      <td>B</td>\n",
              "      <td>4</td>\n",
              "      <td>0.674643</td>\n",
              "      <td>57</td>\n",
              "      <td>0.074516</td>\n",
              "      <td>64</td>\n",
              "      <td>0.060345</td>\n",
              "      <td>68</td>\n",
              "      <td>0.125806</td>\n",
              "      <td>67</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>76</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91</td>\n",
              "      <td>0.001667</td>\n",
              "      <td>83</td>\n",
              "      <td>0.027419</td>\n",
              "      <td>75</td>\n",
              "      <td>145.70</td>\n",
              "      <td>156</td>\n",
              "      <td>180.8</td>\n",
              "      <td>198</td>\n",
              "      <td>39.5</td>\n",
              "      <td>55</td>\n",
              "      <td>200.65</td>\n",
              "      <td>205</td>\n",
              "      <td>77.5</td>\n",
              "      <td>36</td>\n",
              "      <td>121.65</td>\n",
              "      <td>24</td>\n",
              "      <td>52</td>\n",
              "      <td>5.40206</td>\n",
              "      <td>5.8</td>\n",
              "      <td>441.75</td>\n",
              "      <td>477</td>\n",
              "      <td>319.20</td>\n",
              "      <td>251</td>\n",
              "      <td>98.60</td>\n",
              "      <td>18</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Napa Valley</td>\n",
              "      <td>Kongsgaard, The Judge Chardonnay, White, Napa ...</td>\n",
              "      <td>91591</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2015</td>\n",
              "      <td>False</td>\n",
              "      <td>97.27</td>\n",
              "      <td>B+</td>\n",
              "      <td>4</td>\n",
              "      <td>0.176970</td>\n",
              "      <td>56</td>\n",
              "      <td>0.003653</td>\n",
              "      <td>57</td>\n",
              "      <td>0.101661</td>\n",
              "      <td>57</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60</td>\n",
              "      <td>0.010968</td>\n",
              "      <td>65</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>69</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69</td>\n",
              "      <td>0.007210</td>\n",
              "      <td>68</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>65</td>\n",
              "      <td>140.80</td>\n",
              "      <td>150</td>\n",
              "      <td>290.2</td>\n",
              "      <td>289</td>\n",
              "      <td>153.5</td>\n",
              "      <td>150</td>\n",
              "      <td>282.55</td>\n",
              "      <td>273</td>\n",
              "      <td>72.6</td>\n",
              "      <td>40</td>\n",
              "      <td>109.20</td>\n",
              "      <td>31</td>\n",
              "      <td>45</td>\n",
              "      <td>5.90206</td>\n",
              "      <td>6.4</td>\n",
              "      <td>203.00</td>\n",
              "      <td>191</td>\n",
              "      <td>418.85</td>\n",
              "      <td>410</td>\n",
              "      <td>84.00</td>\n",
              "      <td>27</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            appellation  ... soc_100-200cm\n",
              "0  Santa Cruz Mountains  ...            38\n",
              "1           Napa Valley  ...            27\n",
              "2          Sonoma Coast  ...            18\n",
              "3          Sonoma Coast  ...            18\n",
              "4           Napa Valley  ...            27\n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE3o0_0F6i_0",
        "outputId": "3fe48b16-9140-4d7c-9834-1f39a91b3bef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "from google.colab import files\n",
        "White_Soil_ML_df.to_csv('white_weather_soil.csv') \n",
        "files.download('white_weather_soil.csv')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_6fa5e96d-ae00-4e3f-a3c7-bfdc578b889b\", \"white_weather_soil.csv\", 293187)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NORZgQZO59u",
        "outputId": "c76163c9-38e4-406b-c9a4-2c0d5a4d8ab6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "White_Soil_ML_df.dtypes"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "appellation            object\n",
              "wine                   object\n",
              "wine_id                 int64\n",
              "color                  object\n",
              "regions                object\n",
              "country                object\n",
              "vintage                 int64\n",
              "is_primeurs              bool\n",
              "score                 float64\n",
              "confidence_index       object\n",
              "journalist_count        int64\n",
              "avgPrcpFebruary       float64\n",
              "avgTempFebruary         int64\n",
              "avgPrcpMarch          float64\n",
              "avgTempMarch            int64\n",
              "avgPrcpApril          float64\n",
              "avgTempApril            int64\n",
              "avgPrcpMay            float64\n",
              "avgTempMay              int64\n",
              "avgPrcpJune           float64\n",
              "avgTempJune             int64\n",
              "avgPrcpJuly           float64\n",
              "avgTempJuly             int64\n",
              "avgPrcpAugust         float64\n",
              "avgTempAugust           int64\n",
              "avgPrcpSeptember      float64\n",
              "avgTempSeptember        int64\n",
              "avgPrcpOctober        float64\n",
              "avgTempOctober          int64\n",
              "bdod_0-100cm          float64\n",
              "bdod_100-200cm          int64\n",
              "cec_0-100cm           float64\n",
              "cec_100-200cm           int64\n",
              "cfvo_0-100cm          float64\n",
              "cfvo_100-200cm          int64\n",
              "clay_0-100cm          float64\n",
              "clay_100-200cm          int64\n",
              "nitrogen_0-100cm      float64\n",
              "nitrogen_100-200cm      int64\n",
              "ocd_0-100cm           float64\n",
              "ocd_100-200cm           int64\n",
              "ocs_0-30cm              int64\n",
              "phh2o_0-100cm         float64\n",
              "phh2o_100-200cm       float64\n",
              "sand_0-100cm          float64\n",
              "sand_100-200cm          int64\n",
              "silt_0-100cm          float64\n",
              "silt_100-200cm          int64\n",
              "soc_0-100cm           float64\n",
              "soc_100-200cm           int64\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yFuv7s1O59y",
        "outputId": "bdc60078-6dc5-4595-c80c-e403f8fb1bd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "White_Soil_ML_df[\"score\"].astype(int) "
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      92\n",
              "1      92\n",
              "2      92\n",
              "3      91\n",
              "4      97\n",
              "       ..\n",
              "727    87\n",
              "728    91\n",
              "729    93\n",
              "730    86\n",
              "731    88\n",
              "Name: score, Length: 732, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFRIEoeWO590"
      },
      "source": [
        "#Splitting score into good(1) and bad(0) and making it it's own column \"quality\"\n",
        "quality = []\n",
        "\n",
        "for x in White_Soil_ML_df[\"score\"]:\n",
        "  if x >= 91:\n",
        "    quality.append(1)\n",
        "  else:\n",
        "    quality.append(0)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUx-ic_fO592"
      },
      "source": [
        "White_Soil_ML_df[\"quality\"] = quality"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qvx4qNPNO594",
        "outputId": "72cce4a0-4fdb-481f-e550-7a90e414ad81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "White_Soil_ML_df"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>appellation</th>\n",
              "      <th>wine</th>\n",
              "      <th>wine_id</th>\n",
              "      <th>color</th>\n",
              "      <th>regions</th>\n",
              "      <th>country</th>\n",
              "      <th>vintage</th>\n",
              "      <th>is_primeurs</th>\n",
              "      <th>score</th>\n",
              "      <th>confidence_index</th>\n",
              "      <th>journalist_count</th>\n",
              "      <th>avgPrcpFebruary</th>\n",
              "      <th>avgTempFebruary</th>\n",
              "      <th>avgPrcpMarch</th>\n",
              "      <th>avgTempMarch</th>\n",
              "      <th>avgPrcpApril</th>\n",
              "      <th>avgTempApril</th>\n",
              "      <th>avgPrcpMay</th>\n",
              "      <th>avgTempMay</th>\n",
              "      <th>avgPrcpJune</th>\n",
              "      <th>avgTempJune</th>\n",
              "      <th>avgPrcpJuly</th>\n",
              "      <th>avgTempJuly</th>\n",
              "      <th>avgPrcpAugust</th>\n",
              "      <th>avgTempAugust</th>\n",
              "      <th>avgPrcpSeptember</th>\n",
              "      <th>avgTempSeptember</th>\n",
              "      <th>avgPrcpOctober</th>\n",
              "      <th>avgTempOctober</th>\n",
              "      <th>bdod_0-100cm</th>\n",
              "      <th>bdod_100-200cm</th>\n",
              "      <th>cec_0-100cm</th>\n",
              "      <th>cec_100-200cm</th>\n",
              "      <th>cfvo_0-100cm</th>\n",
              "      <th>cfvo_100-200cm</th>\n",
              "      <th>clay_0-100cm</th>\n",
              "      <th>clay_100-200cm</th>\n",
              "      <th>nitrogen_0-100cm</th>\n",
              "      <th>nitrogen_100-200cm</th>\n",
              "      <th>ocd_0-100cm</th>\n",
              "      <th>ocd_100-200cm</th>\n",
              "      <th>ocs_0-30cm</th>\n",
              "      <th>phh2o_0-100cm</th>\n",
              "      <th>phh2o_100-200cm</th>\n",
              "      <th>sand_0-100cm</th>\n",
              "      <th>sand_100-200cm</th>\n",
              "      <th>silt_0-100cm</th>\n",
              "      <th>silt_100-200cm</th>\n",
              "      <th>soc_0-100cm</th>\n",
              "      <th>soc_100-200cm</th>\n",
              "      <th>quality</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Santa Cruz Mountains</td>\n",
              "      <td>Mount Eden Vineyards, Chardonnay, White, Santa...</td>\n",
              "      <td>107658</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2015</td>\n",
              "      <td>False</td>\n",
              "      <td>92.22</td>\n",
              "      <td>B</td>\n",
              "      <td>4</td>\n",
              "      <td>0.174747</td>\n",
              "      <td>58</td>\n",
              "      <td>0.018630</td>\n",
              "      <td>60</td>\n",
              "      <td>0.096254</td>\n",
              "      <td>59</td>\n",
              "      <td>0.002989</td>\n",
              "      <td>60</td>\n",
              "      <td>0.000645</td>\n",
              "      <td>65</td>\n",
              "      <td>0.001132</td>\n",
              "      <td>69</td>\n",
              "      <td>0.000947</td>\n",
              "      <td>70</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>68</td>\n",
              "      <td>0.005581</td>\n",
              "      <td>66</td>\n",
              "      <td>139.75</td>\n",
              "      <td>149</td>\n",
              "      <td>153.4</td>\n",
              "      <td>145</td>\n",
              "      <td>183.50</td>\n",
              "      <td>245</td>\n",
              "      <td>197.50</td>\n",
              "      <td>193</td>\n",
              "      <td>145.70</td>\n",
              "      <td>60</td>\n",
              "      <td>124.95</td>\n",
              "      <td>25</td>\n",
              "      <td>60</td>\n",
              "      <td>5.50206</td>\n",
              "      <td>5.9</td>\n",
              "      <td>442.10</td>\n",
              "      <td>468</td>\n",
              "      <td>324.85</td>\n",
              "      <td>283</td>\n",
              "      <td>128.55</td>\n",
              "      <td>38</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Napa Valley</td>\n",
              "      <td>Pahlmeyer, Napa Valley Chardonnay, White, Napa...</td>\n",
              "      <td>111897</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2015</td>\n",
              "      <td>False</td>\n",
              "      <td>92.83</td>\n",
              "      <td>C+</td>\n",
              "      <td>4</td>\n",
              "      <td>0.176970</td>\n",
              "      <td>56</td>\n",
              "      <td>0.003653</td>\n",
              "      <td>57</td>\n",
              "      <td>0.101661</td>\n",
              "      <td>57</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60</td>\n",
              "      <td>0.010968</td>\n",
              "      <td>65</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>69</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69</td>\n",
              "      <td>0.007210</td>\n",
              "      <td>68</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>65</td>\n",
              "      <td>140.80</td>\n",
              "      <td>150</td>\n",
              "      <td>290.2</td>\n",
              "      <td>289</td>\n",
              "      <td>153.50</td>\n",
              "      <td>150</td>\n",
              "      <td>282.55</td>\n",
              "      <td>273</td>\n",
              "      <td>72.60</td>\n",
              "      <td>40</td>\n",
              "      <td>109.20</td>\n",
              "      <td>31</td>\n",
              "      <td>45</td>\n",
              "      <td>5.90206</td>\n",
              "      <td>6.4</td>\n",
              "      <td>203.00</td>\n",
              "      <td>191</td>\n",
              "      <td>418.85</td>\n",
              "      <td>410</td>\n",
              "      <td>84.00</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Sonoma Coast</td>\n",
              "      <td>Marcassin, Lorenzo Vineyard Chardonnay, White,...</td>\n",
              "      <td>101640</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>1993</td>\n",
              "      <td>False</td>\n",
              "      <td>92.07</td>\n",
              "      <td>C</td>\n",
              "      <td>3</td>\n",
              "      <td>0.275357</td>\n",
              "      <td>58</td>\n",
              "      <td>0.086129</td>\n",
              "      <td>67</td>\n",
              "      <td>0.050667</td>\n",
              "      <td>68</td>\n",
              "      <td>0.068929</td>\n",
              "      <td>73</td>\n",
              "      <td>0.029333</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>85</td>\n",
              "      <td>0.058710</td>\n",
              "      <td>77</td>\n",
              "      <td>145.70</td>\n",
              "      <td>156</td>\n",
              "      <td>180.8</td>\n",
              "      <td>198</td>\n",
              "      <td>39.50</td>\n",
              "      <td>55</td>\n",
              "      <td>200.65</td>\n",
              "      <td>205</td>\n",
              "      <td>77.50</td>\n",
              "      <td>36</td>\n",
              "      <td>121.65</td>\n",
              "      <td>24</td>\n",
              "      <td>52</td>\n",
              "      <td>5.40206</td>\n",
              "      <td>5.8</td>\n",
              "      <td>441.75</td>\n",
              "      <td>477</td>\n",
              "      <td>319.20</td>\n",
              "      <td>251</td>\n",
              "      <td>98.60</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Sonoma Coast</td>\n",
              "      <td>Marcassin, Lorenzo Vineyard Chardonnay, White,...</td>\n",
              "      <td>101640</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>1998</td>\n",
              "      <td>False</td>\n",
              "      <td>91.74</td>\n",
              "      <td>B</td>\n",
              "      <td>4</td>\n",
              "      <td>0.674643</td>\n",
              "      <td>57</td>\n",
              "      <td>0.074516</td>\n",
              "      <td>64</td>\n",
              "      <td>0.060345</td>\n",
              "      <td>68</td>\n",
              "      <td>0.125806</td>\n",
              "      <td>67</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>76</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91</td>\n",
              "      <td>0.001667</td>\n",
              "      <td>83</td>\n",
              "      <td>0.027419</td>\n",
              "      <td>75</td>\n",
              "      <td>145.70</td>\n",
              "      <td>156</td>\n",
              "      <td>180.8</td>\n",
              "      <td>198</td>\n",
              "      <td>39.50</td>\n",
              "      <td>55</td>\n",
              "      <td>200.65</td>\n",
              "      <td>205</td>\n",
              "      <td>77.50</td>\n",
              "      <td>36</td>\n",
              "      <td>121.65</td>\n",
              "      <td>24</td>\n",
              "      <td>52</td>\n",
              "      <td>5.40206</td>\n",
              "      <td>5.8</td>\n",
              "      <td>441.75</td>\n",
              "      <td>477</td>\n",
              "      <td>319.20</td>\n",
              "      <td>251</td>\n",
              "      <td>98.60</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Napa Valley</td>\n",
              "      <td>Kongsgaard, The Judge Chardonnay, White, Napa ...</td>\n",
              "      <td>91591</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>2015</td>\n",
              "      <td>False</td>\n",
              "      <td>97.27</td>\n",
              "      <td>B+</td>\n",
              "      <td>4</td>\n",
              "      <td>0.176970</td>\n",
              "      <td>56</td>\n",
              "      <td>0.003653</td>\n",
              "      <td>57</td>\n",
              "      <td>0.101661</td>\n",
              "      <td>57</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60</td>\n",
              "      <td>0.010968</td>\n",
              "      <td>65</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>69</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69</td>\n",
              "      <td>0.007210</td>\n",
              "      <td>68</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>65</td>\n",
              "      <td>140.80</td>\n",
              "      <td>150</td>\n",
              "      <td>290.2</td>\n",
              "      <td>289</td>\n",
              "      <td>153.50</td>\n",
              "      <td>150</td>\n",
              "      <td>282.55</td>\n",
              "      <td>273</td>\n",
              "      <td>72.60</td>\n",
              "      <td>40</td>\n",
              "      <td>109.20</td>\n",
              "      <td>31</td>\n",
              "      <td>45</td>\n",
              "      <td>5.90206</td>\n",
              "      <td>6.4</td>\n",
              "      <td>203.00</td>\n",
              "      <td>191</td>\n",
              "      <td>418.85</td>\n",
              "      <td>410</td>\n",
              "      <td>84.00</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>727</th>\n",
              "      <td>Carneros</td>\n",
              "      <td>Truchard Vineyards, Chardonnay, White, Carneros</td>\n",
              "      <td>136966</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>1996</td>\n",
              "      <td>False</td>\n",
              "      <td>87.68</td>\n",
              "      <td>C</td>\n",
              "      <td>3</td>\n",
              "      <td>0.312000</td>\n",
              "      <td>62</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>68</td>\n",
              "      <td>0.116333</td>\n",
              "      <td>71</td>\n",
              "      <td>0.108710</td>\n",
              "      <td>76</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>85</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>90</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>90</td>\n",
              "      <td>0.005667</td>\n",
              "      <td>83</td>\n",
              "      <td>0.070000</td>\n",
              "      <td>76</td>\n",
              "      <td>160.40</td>\n",
              "      <td>161</td>\n",
              "      <td>215.4</td>\n",
              "      <td>233</td>\n",
              "      <td>30.15</td>\n",
              "      <td>25</td>\n",
              "      <td>216.10</td>\n",
              "      <td>219</td>\n",
              "      <td>60.15</td>\n",
              "      <td>36</td>\n",
              "      <td>117.25</td>\n",
              "      <td>38</td>\n",
              "      <td>36</td>\n",
              "      <td>5.80206</td>\n",
              "      <td>6.4</td>\n",
              "      <td>317.75</td>\n",
              "      <td>340</td>\n",
              "      <td>408.60</td>\n",
              "      <td>401</td>\n",
              "      <td>57.90</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>728</th>\n",
              "      <td>Sonoma County</td>\n",
              "      <td>Peter Michael Winery, Belle Cote Chardonnay, W...</td>\n",
              "      <td>114819</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>1996</td>\n",
              "      <td>False</td>\n",
              "      <td>91.76</td>\n",
              "      <td>C+</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>145.00</td>\n",
              "      <td>150</td>\n",
              "      <td>184.4</td>\n",
              "      <td>190</td>\n",
              "      <td>105.50</td>\n",
              "      <td>110</td>\n",
              "      <td>262.80</td>\n",
              "      <td>298</td>\n",
              "      <td>84.70</td>\n",
              "      <td>36</td>\n",
              "      <td>121.60</td>\n",
              "      <td>29</td>\n",
              "      <td>55</td>\n",
              "      <td>5.20206</td>\n",
              "      <td>5.7</td>\n",
              "      <td>352.40</td>\n",
              "      <td>337</td>\n",
              "      <td>371.65</td>\n",
              "      <td>346</td>\n",
              "      <td>110.95</td>\n",
              "      <td>27</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>729</th>\n",
              "      <td>Carneros</td>\n",
              "      <td>Kistler Vineyards, Hudson Vineyard Chardonnay,...</td>\n",
              "      <td>91298</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>1995</td>\n",
              "      <td>False</td>\n",
              "      <td>93.90</td>\n",
              "      <td>A</td>\n",
              "      <td>3</td>\n",
              "      <td>0.029286</td>\n",
              "      <td>63</td>\n",
              "      <td>0.428710</td>\n",
              "      <td>63</td>\n",
              "      <td>0.044333</td>\n",
              "      <td>68</td>\n",
              "      <td>0.060968</td>\n",
              "      <td>72</td>\n",
              "      <td>0.034667</td>\n",
              "      <td>80</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>88</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>89</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>84</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>80</td>\n",
              "      <td>160.40</td>\n",
              "      <td>161</td>\n",
              "      <td>215.4</td>\n",
              "      <td>233</td>\n",
              "      <td>30.15</td>\n",
              "      <td>25</td>\n",
              "      <td>216.10</td>\n",
              "      <td>219</td>\n",
              "      <td>60.15</td>\n",
              "      <td>36</td>\n",
              "      <td>117.25</td>\n",
              "      <td>38</td>\n",
              "      <td>36</td>\n",
              "      <td>5.80206</td>\n",
              "      <td>6.4</td>\n",
              "      <td>317.75</td>\n",
              "      <td>340</td>\n",
              "      <td>408.60</td>\n",
              "      <td>401</td>\n",
              "      <td>57.90</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>730</th>\n",
              "      <td>Los Carneros</td>\n",
              "      <td>Joseph Phelps Vineyards, Carneros Chardonnay, ...</td>\n",
              "      <td>89562</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>1996</td>\n",
              "      <td>False</td>\n",
              "      <td>86.86</td>\n",
              "      <td>A</td>\n",
              "      <td>3</td>\n",
              "      <td>0.368800</td>\n",
              "      <td>52</td>\n",
              "      <td>0.087500</td>\n",
              "      <td>54</td>\n",
              "      <td>0.127000</td>\n",
              "      <td>56</td>\n",
              "      <td>0.120000</td>\n",
              "      <td>60</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>64</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>66</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>63</td>\n",
              "      <td>0.062581</td>\n",
              "      <td>60</td>\n",
              "      <td>158.50</td>\n",
              "      <td>160</td>\n",
              "      <td>218.4</td>\n",
              "      <td>224</td>\n",
              "      <td>80.25</td>\n",
              "      <td>60</td>\n",
              "      <td>196.15</td>\n",
              "      <td>190</td>\n",
              "      <td>60.85</td>\n",
              "      <td>35</td>\n",
              "      <td>118.15</td>\n",
              "      <td>37</td>\n",
              "      <td>34</td>\n",
              "      <td>5.60206</td>\n",
              "      <td>6.2</td>\n",
              "      <td>353.05</td>\n",
              "      <td>365</td>\n",
              "      <td>362.85</td>\n",
              "      <td>391</td>\n",
              "      <td>63.50</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>731</th>\n",
              "      <td>Sonoma County</td>\n",
              "      <td>Ferrari-Carano, Fume Blanc, White, Sonoma County</td>\n",
              "      <td>73121</td>\n",
              "      <td>White</td>\n",
              "      <td>California</td>\n",
              "      <td>Usa</td>\n",
              "      <td>1993</td>\n",
              "      <td>False</td>\n",
              "      <td>88.14</td>\n",
              "      <td>B+</td>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0</td>\n",
              "      <td>145.00</td>\n",
              "      <td>150</td>\n",
              "      <td>184.4</td>\n",
              "      <td>190</td>\n",
              "      <td>105.50</td>\n",
              "      <td>110</td>\n",
              "      <td>262.80</td>\n",
              "      <td>298</td>\n",
              "      <td>84.70</td>\n",
              "      <td>36</td>\n",
              "      <td>121.60</td>\n",
              "      <td>29</td>\n",
              "      <td>55</td>\n",
              "      <td>5.20206</td>\n",
              "      <td>5.7</td>\n",
              "      <td>352.40</td>\n",
              "      <td>337</td>\n",
              "      <td>371.65</td>\n",
              "      <td>346</td>\n",
              "      <td>110.95</td>\n",
              "      <td>27</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>732 rows × 51 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "              appellation  ... quality\n",
              "0    Santa Cruz Mountains  ...       1\n",
              "1             Napa Valley  ...       1\n",
              "2            Sonoma Coast  ...       1\n",
              "3            Sonoma Coast  ...       1\n",
              "4             Napa Valley  ...       1\n",
              "..                    ...  ...     ...\n",
              "727              Carneros  ...       0\n",
              "728         Sonoma County  ...       1\n",
              "729              Carneros  ...       1\n",
              "730          Los Carneros  ...       0\n",
              "731         Sonoma County  ...       0\n",
              "\n",
              "[732 rows x 51 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FBb4KAPO596",
        "outputId": "56e69b4c-e0d3-4bf0-e388-9da831ea6514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Generate our categorical variable list\n",
        "White_Wine_cat = White_Soil_ML_df.dtypes[White_Soil_ML_df.dtypes == \"object\"].index.tolist()\n",
        "\n",
        "# Check the number of unique values in each column\n",
        "White_Soil_ML_df[White_Wine_cat].nunique()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "appellation          22\n",
              "wine                163\n",
              "color                 1\n",
              "regions               3\n",
              "country               1\n",
              "confidence_index      6\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4mB3r6TO598",
        "outputId": "0d633d8c-3095-48b3-c9e7-94db2d8cbaee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check the unique value counts to see if binning is required for Appellation\n",
        "Appellation_Count = White_Soil_ML_df.appellation.value_counts()\n",
        "Appellation_Count.head(10)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Napa Valley             229\n",
              "Carneros                112\n",
              "Russian River Valley     92\n",
              "Sonoma County            80\n",
              "Knights Valley           49\n",
              "Sonoma Coast             37\n",
              "Sonoma Mountain          29\n",
              "Santa Cruz Mountains     19\n",
              "Columbia Valley          17\n",
              "Eola-Amity Hills         13\n",
              "Name: appellation, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg62g5TjO5-A",
        "outputId": "e1dd90a1-41c3-4489-ca6e-483bd6296891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "# Visualize the Appellation_Count\n",
        "Appellation_Count.plot.density()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f694c733128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pTwMSKWO5-C"
      },
      "source": [
        "# Determine which values to replace for \n",
        "Appellation_Bin =  list(Appellation_Count[Appellation_Count < 200].index)\n",
        "# Replace in DataFrame\n",
        "for type in Appellation_Bin:\n",
        "    White_Soil_ML_df.appellation = White_Soil_ML_df.appellation.replace(type,\"Other\")"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55cb4HnEO5-G",
        "outputId": "73490b31-81a7-4f9f-a23a-2c9b7eb6387a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Check to make sure binning was successful for Appellation\n",
        "White_Soil_ML_df.appellation.value_counts()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Other          503\n",
              "Napa Valley    229\n",
              "Name: appellation, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UoMqlVkUO5-I"
      },
      "source": [
        "# White_Soil_ML_df[White_Soil_ML_df.appellation != 'Other']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLCf6JriO5-L",
        "outputId": "7b347c8f-05bd-4437-a98b-9f828e276c32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "# Create the OneHotEncoder instance\n",
        "enc = OneHotEncoder(sparse=False)\n",
        "\n",
        "# Fit the encoder and produce encoded DataFrame\n",
        "White_Wine_encode_df = pd.DataFrame(enc.fit_transform(White_Soil_ML_df[White_Wine_cat]))\n",
        "\n",
        "# Rename encoded columns\n",
        "White_Wine_encode_df.columns = enc.get_feature_names(White_Wine_cat)\n",
        "White_Wine_encode_df.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>appellation_Napa Valley</th>\n",
              "      <th>appellation_Other</th>\n",
              "      <th>wine_Alpha Omega, Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Alpha Omega, Reserve Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Apsara Cellars, 'Rivers Reach' Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Araujo Estate, Eisele Vineyard Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Aubert Wines, Hudson Vineyard Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Aubert Wines, Larry Hyde &amp; Sons Vineyard Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Aubert Wines, Ritchie Vineyard Chardonnay, White, Sonoma Coast</th>\n",
              "      <th>wine_Aubert Wines, Sugar Shack Estate Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Aubert Wines, Uv-Sl Vineyards Chardonnay, White, Sonoma Coast</th>\n",
              "      <th>wine_Beaulieu Vineyard Bv, Reserve Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Beringer Vineyards, 'Luminus' Chardonnay, White, Oak Knoll District</th>\n",
              "      <th>wine_Beringer Vineyards, Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Beringer Vineyards, Private Reserve Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Cakebread Cellars, Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Cakebread Cellars, Reserve Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Cakebread Cellars, Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Chappellet, Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Charles Krug Peter Mondavi Family, Sauvignon Blanc, White, St Helena</th>\n",
              "      <th>wine_Chateau Montelena, Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Cliff Lede Vineyards, Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Clos Du Val, Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Clos Du Val, Winemaker'S Signature Series Three Graces White Blend, White, Napa Valley</th>\n",
              "      <th>wine_Crossbarn By Paul Hobbs, Chardonnay, White, Sonoma Coast</th>\n",
              "      <th>wine_Cuvaison, Ats Selection Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Cuvaison, Carneros Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Delille Cellars, Chaleur Estate Blanc, White, Columbia Valley</th>\n",
              "      <th>wine_Delille Cellars, Doyenne Metier Blanc, White, Red Mountain</th>\n",
              "      <th>wine_Delille Cellars, Doyenne Roussanne, White, Red Mountain</th>\n",
              "      <th>wine_Domaine Serene, 'Dijon Clones - Cote Sud Vineyard' Chardonnay, White, Willamette Valley</th>\n",
              "      <th>wine_Domaine Serene, 'Evenstad Reserve' Chardonnay, White, Dundee Hills</th>\n",
              "      <th>wine_Domaine Serene, Clos Du Soleil Vineyard Chardonnay, White, Dundee Hills</th>\n",
              "      <th>wine_Domaine Serene, Etoile Vineyard Chardonnay, White, Dundee Hills</th>\n",
              "      <th>wine_Duckhorn Vineyards, Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Duckhorn Vineyards, Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Dumol, Clare Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Etude, Grace Benoist Ranch Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Etude, Pinot Gris, White, Carneros</th>\n",
              "      <th>wine_Evening Land, Gold Label Seven Springs Vineyard Chardonnay, White, Eola-Amity Hills</th>\n",
              "      <th>...</th>\n",
              "      <th>wine_Robert Foley Vineyards, Pinot Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Robert Mondavi Winery, Carneros Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Robert Mondavi Winery, Napa Valley Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Rombauer Vineyards, Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Rudd, Bacigalupi Vineyard Chardonnay, White, Russian River Valley</th>\n",
              "      <th>wine_Rudd, Mount Veeder Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Saintsbury, Brown Ranch Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Saintsbury, Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Saintsbury, Reserve Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Screaming Eagle, Sauvignon Blanc, White, Oakville</th>\n",
              "      <th>wine_Shafer Vineyards, Red Shoulder Ranch Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Silverado Vineyards Estate, Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Silverado Vineyards, 'Vineburg Vineyard' Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Silverado Vineyards, Miller Ranch Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Smith Madrone, Riesling, White, Spring Mountain District</th>\n",
              "      <th>wine_Spottswoode, Sauvignon Blanc, White, Napa County</th>\n",
              "      <th>wine_St. Clement Vineyards, Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Stag'S Leap Wine Cellars, Arcadia Vineyard Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Stag'S Leap Wine Cellars, Aveta Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Stag'S Leap Wine Cellars, Karia Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Stag'S Leap Wine Cellars, Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Stags' Leap Winery, Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Trefethen Family Vineyards, Chardonnay, White, Oak Knoll District</th>\n",
              "      <th>wine_Truchard Vineyards, Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Truchard Vineyards, Roussanne, White, Carneros</th>\n",
              "      <th>wine_Turnbull Wine Cellars, Sauvignon Blanc, White, Oakville</th>\n",
              "      <th>wine_Twomey Cellars, Sauvignon Blanc, White, Napa County</th>\n",
              "      <th>wine_Venge Vineyards, Maldonado Vineyard Dijon Clones Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Vine Cliff Winery, Chardonnay, White, Los Carneros</th>\n",
              "      <th>color_White</th>\n",
              "      <th>regions_California</th>\n",
              "      <th>regions_Oregon</th>\n",
              "      <th>regions_Washington</th>\n",
              "      <th>country_Usa</th>\n",
              "      <th>confidence_index_A</th>\n",
              "      <th>confidence_index_A+</th>\n",
              "      <th>confidence_index_B</th>\n",
              "      <th>confidence_index_B+</th>\n",
              "      <th>confidence_index_C</th>\n",
              "      <th>confidence_index_C+</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 176 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   appellation_Napa Valley  ...  confidence_index_C+\n",
              "0                      0.0  ...                  0.0\n",
              "1                      1.0  ...                  1.0\n",
              "2                      0.0  ...                  0.0\n",
              "3                      0.0  ...                  0.0\n",
              "4                      1.0  ...                  0.0\n",
              "\n",
              "[5 rows x 176 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iF4QT7qzO5-N",
        "outputId": "ce8f1204-8490-4aaa-a99a-a66902f6cb7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        }
      },
      "source": [
        "# Merge one-hot encoded features and drop the originals\n",
        "White_Soil_ML_df = White_Soil_ML_df.merge(White_Wine_encode_df,left_index=True, right_index=True)\n",
        "White_Soil_ML_df = White_Soil_ML_df.drop(White_Wine_cat,1)\n",
        "White_Soil_ML_df.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>wine_id</th>\n",
              "      <th>vintage</th>\n",
              "      <th>is_primeurs</th>\n",
              "      <th>score</th>\n",
              "      <th>journalist_count</th>\n",
              "      <th>avgPrcpFebruary</th>\n",
              "      <th>avgTempFebruary</th>\n",
              "      <th>avgPrcpMarch</th>\n",
              "      <th>avgTempMarch</th>\n",
              "      <th>avgPrcpApril</th>\n",
              "      <th>avgTempApril</th>\n",
              "      <th>avgPrcpMay</th>\n",
              "      <th>avgTempMay</th>\n",
              "      <th>avgPrcpJune</th>\n",
              "      <th>avgTempJune</th>\n",
              "      <th>avgPrcpJuly</th>\n",
              "      <th>avgTempJuly</th>\n",
              "      <th>avgPrcpAugust</th>\n",
              "      <th>avgTempAugust</th>\n",
              "      <th>avgPrcpSeptember</th>\n",
              "      <th>avgTempSeptember</th>\n",
              "      <th>avgPrcpOctober</th>\n",
              "      <th>avgTempOctober</th>\n",
              "      <th>bdod_0-100cm</th>\n",
              "      <th>bdod_100-200cm</th>\n",
              "      <th>cec_0-100cm</th>\n",
              "      <th>cec_100-200cm</th>\n",
              "      <th>cfvo_0-100cm</th>\n",
              "      <th>cfvo_100-200cm</th>\n",
              "      <th>clay_0-100cm</th>\n",
              "      <th>clay_100-200cm</th>\n",
              "      <th>nitrogen_0-100cm</th>\n",
              "      <th>nitrogen_100-200cm</th>\n",
              "      <th>ocd_0-100cm</th>\n",
              "      <th>ocd_100-200cm</th>\n",
              "      <th>ocs_0-30cm</th>\n",
              "      <th>phh2o_0-100cm</th>\n",
              "      <th>phh2o_100-200cm</th>\n",
              "      <th>sand_0-100cm</th>\n",
              "      <th>sand_100-200cm</th>\n",
              "      <th>...</th>\n",
              "      <th>wine_Robert Foley Vineyards, Pinot Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Robert Mondavi Winery, Carneros Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Robert Mondavi Winery, Napa Valley Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Rombauer Vineyards, Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Rudd, Bacigalupi Vineyard Chardonnay, White, Russian River Valley</th>\n",
              "      <th>wine_Rudd, Mount Veeder Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Saintsbury, Brown Ranch Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Saintsbury, Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Saintsbury, Reserve Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Screaming Eagle, Sauvignon Blanc, White, Oakville</th>\n",
              "      <th>wine_Shafer Vineyards, Red Shoulder Ranch Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Silverado Vineyards Estate, Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Silverado Vineyards, 'Vineburg Vineyard' Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Silverado Vineyards, Miller Ranch Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Smith Madrone, Riesling, White, Spring Mountain District</th>\n",
              "      <th>wine_Spottswoode, Sauvignon Blanc, White, Napa County</th>\n",
              "      <th>wine_St. Clement Vineyards, Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Stag'S Leap Wine Cellars, Arcadia Vineyard Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Stag'S Leap Wine Cellars, Aveta Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Stag'S Leap Wine Cellars, Karia Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Stag'S Leap Wine Cellars, Sauvignon Blanc, White, Napa Valley</th>\n",
              "      <th>wine_Stags' Leap Winery, Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Trefethen Family Vineyards, Chardonnay, White, Oak Knoll District</th>\n",
              "      <th>wine_Truchard Vineyards, Chardonnay, White, Carneros</th>\n",
              "      <th>wine_Truchard Vineyards, Roussanne, White, Carneros</th>\n",
              "      <th>wine_Turnbull Wine Cellars, Sauvignon Blanc, White, Oakville</th>\n",
              "      <th>wine_Twomey Cellars, Sauvignon Blanc, White, Napa County</th>\n",
              "      <th>wine_Venge Vineyards, Maldonado Vineyard Dijon Clones Chardonnay, White, Napa Valley</th>\n",
              "      <th>wine_Vine Cliff Winery, Chardonnay, White, Los Carneros</th>\n",
              "      <th>color_White</th>\n",
              "      <th>regions_California</th>\n",
              "      <th>regions_Oregon</th>\n",
              "      <th>regions_Washington</th>\n",
              "      <th>country_Usa</th>\n",
              "      <th>confidence_index_A</th>\n",
              "      <th>confidence_index_A+</th>\n",
              "      <th>confidence_index_B</th>\n",
              "      <th>confidence_index_B+</th>\n",
              "      <th>confidence_index_C</th>\n",
              "      <th>confidence_index_C+</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>107658</td>\n",
              "      <td>2015</td>\n",
              "      <td>False</td>\n",
              "      <td>92.22</td>\n",
              "      <td>4</td>\n",
              "      <td>0.174747</td>\n",
              "      <td>58</td>\n",
              "      <td>0.018630</td>\n",
              "      <td>60</td>\n",
              "      <td>0.096254</td>\n",
              "      <td>59</td>\n",
              "      <td>0.002989</td>\n",
              "      <td>60</td>\n",
              "      <td>0.000645</td>\n",
              "      <td>65</td>\n",
              "      <td>0.001132</td>\n",
              "      <td>69</td>\n",
              "      <td>0.000947</td>\n",
              "      <td>70</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>68</td>\n",
              "      <td>0.005581</td>\n",
              "      <td>66</td>\n",
              "      <td>139.75</td>\n",
              "      <td>149</td>\n",
              "      <td>153.4</td>\n",
              "      <td>145</td>\n",
              "      <td>183.5</td>\n",
              "      <td>245</td>\n",
              "      <td>197.50</td>\n",
              "      <td>193</td>\n",
              "      <td>145.7</td>\n",
              "      <td>60</td>\n",
              "      <td>124.95</td>\n",
              "      <td>25</td>\n",
              "      <td>60</td>\n",
              "      <td>5.50206</td>\n",
              "      <td>5.9</td>\n",
              "      <td>442.10</td>\n",
              "      <td>468</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>111897</td>\n",
              "      <td>2015</td>\n",
              "      <td>False</td>\n",
              "      <td>92.83</td>\n",
              "      <td>4</td>\n",
              "      <td>0.176970</td>\n",
              "      <td>56</td>\n",
              "      <td>0.003653</td>\n",
              "      <td>57</td>\n",
              "      <td>0.101661</td>\n",
              "      <td>57</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60</td>\n",
              "      <td>0.010968</td>\n",
              "      <td>65</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>69</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69</td>\n",
              "      <td>0.007210</td>\n",
              "      <td>68</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>65</td>\n",
              "      <td>140.80</td>\n",
              "      <td>150</td>\n",
              "      <td>290.2</td>\n",
              "      <td>289</td>\n",
              "      <td>153.5</td>\n",
              "      <td>150</td>\n",
              "      <td>282.55</td>\n",
              "      <td>273</td>\n",
              "      <td>72.6</td>\n",
              "      <td>40</td>\n",
              "      <td>109.20</td>\n",
              "      <td>31</td>\n",
              "      <td>45</td>\n",
              "      <td>5.90206</td>\n",
              "      <td>6.4</td>\n",
              "      <td>203.00</td>\n",
              "      <td>191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>101640</td>\n",
              "      <td>1993</td>\n",
              "      <td>False</td>\n",
              "      <td>92.07</td>\n",
              "      <td>3</td>\n",
              "      <td>0.275357</td>\n",
              "      <td>58</td>\n",
              "      <td>0.086129</td>\n",
              "      <td>67</td>\n",
              "      <td>0.050667</td>\n",
              "      <td>68</td>\n",
              "      <td>0.068929</td>\n",
              "      <td>73</td>\n",
              "      <td>0.029333</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>85</td>\n",
              "      <td>0.058710</td>\n",
              "      <td>77</td>\n",
              "      <td>145.70</td>\n",
              "      <td>156</td>\n",
              "      <td>180.8</td>\n",
              "      <td>198</td>\n",
              "      <td>39.5</td>\n",
              "      <td>55</td>\n",
              "      <td>200.65</td>\n",
              "      <td>205</td>\n",
              "      <td>77.5</td>\n",
              "      <td>36</td>\n",
              "      <td>121.65</td>\n",
              "      <td>24</td>\n",
              "      <td>52</td>\n",
              "      <td>5.40206</td>\n",
              "      <td>5.8</td>\n",
              "      <td>441.75</td>\n",
              "      <td>477</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>101640</td>\n",
              "      <td>1998</td>\n",
              "      <td>False</td>\n",
              "      <td>91.74</td>\n",
              "      <td>4</td>\n",
              "      <td>0.674643</td>\n",
              "      <td>57</td>\n",
              "      <td>0.074516</td>\n",
              "      <td>64</td>\n",
              "      <td>0.060345</td>\n",
              "      <td>68</td>\n",
              "      <td>0.125806</td>\n",
              "      <td>67</td>\n",
              "      <td>0.005000</td>\n",
              "      <td>76</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>86</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>91</td>\n",
              "      <td>0.001667</td>\n",
              "      <td>83</td>\n",
              "      <td>0.027419</td>\n",
              "      <td>75</td>\n",
              "      <td>145.70</td>\n",
              "      <td>156</td>\n",
              "      <td>180.8</td>\n",
              "      <td>198</td>\n",
              "      <td>39.5</td>\n",
              "      <td>55</td>\n",
              "      <td>200.65</td>\n",
              "      <td>205</td>\n",
              "      <td>77.5</td>\n",
              "      <td>36</td>\n",
              "      <td>121.65</td>\n",
              "      <td>24</td>\n",
              "      <td>52</td>\n",
              "      <td>5.40206</td>\n",
              "      <td>5.8</td>\n",
              "      <td>441.75</td>\n",
              "      <td>477</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>91591</td>\n",
              "      <td>2015</td>\n",
              "      <td>False</td>\n",
              "      <td>97.27</td>\n",
              "      <td>4</td>\n",
              "      <td>0.176970</td>\n",
              "      <td>56</td>\n",
              "      <td>0.003653</td>\n",
              "      <td>57</td>\n",
              "      <td>0.101661</td>\n",
              "      <td>57</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60</td>\n",
              "      <td>0.010968</td>\n",
              "      <td>65</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>69</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>69</td>\n",
              "      <td>0.007210</td>\n",
              "      <td>68</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>65</td>\n",
              "      <td>140.80</td>\n",
              "      <td>150</td>\n",
              "      <td>290.2</td>\n",
              "      <td>289</td>\n",
              "      <td>153.5</td>\n",
              "      <td>150</td>\n",
              "      <td>282.55</td>\n",
              "      <td>273</td>\n",
              "      <td>72.6</td>\n",
              "      <td>40</td>\n",
              "      <td>109.20</td>\n",
              "      <td>31</td>\n",
              "      <td>45</td>\n",
              "      <td>5.90206</td>\n",
              "      <td>6.4</td>\n",
              "      <td>203.00</td>\n",
              "      <td>191</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 221 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   wine_id  vintage  ...  confidence_index_C  confidence_index_C+\n",
              "0   107658     2015  ...                 0.0                  0.0\n",
              "1   111897     2015  ...                 0.0                  1.0\n",
              "2   101640     1993  ...                 1.0                  0.0\n",
              "3   101640     1998  ...                 0.0                  0.0\n",
              "4    91591     2015  ...                 0.0                  0.0\n",
              "\n",
              "[5 rows x 221 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uPm_kDc8Fon"
      },
      "source": [
        "## ***Wine Only - Drop All weather and soil columns***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCbZATbmO5-Q"
      },
      "source": [
        "# Split our preprocessed data into our features and target arrays\n",
        "y = White_Soil_ML_df[\"quality\"].values\n",
        "X = White_Soil_ML_df.drop([\"quality\", \"is_primeurs\", \"journalist_count\", \"confidence_index_A\",\"confidence_index_A+\", \"confidence_index_B\", \"confidence_index_B+\",\"confidence_index_C\", \"confidence_index_C+\", 'avgPrcpFebruary',\n",
        " 'avgTempFebruary',\n",
        " 'avgPrcpMarch',\n",
        " 'avgTempMarch',\n",
        " 'avgPrcpApril',\n",
        " 'avgTempApril',\n",
        " 'avgPrcpMay',\n",
        " 'avgTempMay',\n",
        " 'avgPrcpJune',\n",
        " 'avgTempJune',\n",
        " 'avgPrcpJuly',\n",
        " 'avgTempJuly',\n",
        " 'avgPrcpAugust',\n",
        " 'avgTempAugust',\n",
        " 'avgPrcpSeptember',\n",
        " 'avgTempSeptember',\n",
        " 'avgPrcpOctober',\n",
        " 'avgTempOctober',\n",
        " 'bdod_0-100cm',\n",
        " 'bdod_100-200cm',\n",
        " 'cec_0-100cm',\n",
        " 'cec_100-200cm',\n",
        " 'cfvo_0-100cm',\n",
        " 'cfvo_100-200cm',\n",
        " 'clay_0-100cm',\n",
        " 'clay_100-200cm',\n",
        " 'nitrogen_0-100cm',\n",
        " 'nitrogen_100-200cm',\n",
        " 'ocd_0-100cm',\n",
        " 'ocd_100-200cm',\n",
        " 'ocs_0-30cm',\n",
        " 'phh2o_0-100cm',\n",
        " 'phh2o_100-200cm',\n",
        " 'sand_0-100cm',\n",
        " 'sand_100-200cm',\n",
        " 'silt_0-100cm',\n",
        " 'silt_100-200cm',\n",
        " 'soc_0-100cm',\n",
        " 'soc_100-200cm'],1).values"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcVEIlvIO5-S"
      },
      "source": [
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X)\n",
        "X_scaled = X_scaler.transform(X)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE4Fr6BaJ-Ep"
      },
      "source": [
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, random_state=45)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G73C39aiO5-V"
      },
      "source": [
        "###***Deep Learning Neural Netwrok for training data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u7Lvg2HO5-V",
        "outputId": "bfda1c9c-b8a4-4fa2-ad50-267872b45ac0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define the model - deep neural net\n",
        "number_input_features = len(X_train_scaled[-1])\n",
        "hidden_nodes_layer1 =  24\n",
        "hidden_nodes_layer2 = 12\n",
        "\n",
        "# metrics = [\n",
        "#     tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
        "#     tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
        "#     tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
        "#     tf.keras.metrics.TruePositives(name=\"tp\"),\n",
        "#     tf.keras.metrics.Precision(name=\"precision\"),\n",
        "#     tf.keras.metrics.Recall(name=\"recall\"),\n",
        "# ]\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile the Sequential model together and customize metrics\n",
        "nn.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train_scaled, y_train, epochs=50, validation_split=0.5,verbose=2)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "9/9 - 0s - loss: 0.8219 - accuracy: 0.5182 - val_loss: 0.7762 - val_accuracy: 0.4909\n",
            "Epoch 2/50\n",
            "9/9 - 0s - loss: 0.7071 - accuracy: 0.5584 - val_loss: 0.7088 - val_accuracy: 0.5382\n",
            "Epoch 3/50\n",
            "9/9 - 0s - loss: 0.6401 - accuracy: 0.6314 - val_loss: 0.6588 - val_accuracy: 0.6036\n",
            "Epoch 4/50\n",
            "9/9 - 0s - loss: 0.5882 - accuracy: 0.6788 - val_loss: 0.6253 - val_accuracy: 0.6182\n",
            "Epoch 5/50\n",
            "9/9 - 0s - loss: 0.5498 - accuracy: 0.7226 - val_loss: 0.5962 - val_accuracy: 0.6473\n",
            "Epoch 6/50\n",
            "9/9 - 0s - loss: 0.5144 - accuracy: 0.7737 - val_loss: 0.5725 - val_accuracy: 0.7091\n",
            "Epoch 7/50\n",
            "9/9 - 0s - loss: 0.4858 - accuracy: 0.8066 - val_loss: 0.5501 - val_accuracy: 0.7455\n",
            "Epoch 8/50\n",
            "9/9 - 0s - loss: 0.4590 - accuracy: 0.8285 - val_loss: 0.5303 - val_accuracy: 0.7636\n",
            "Epoch 9/50\n",
            "9/9 - 0s - loss: 0.4350 - accuracy: 0.8358 - val_loss: 0.5128 - val_accuracy: 0.7782\n",
            "Epoch 10/50\n",
            "9/9 - 0s - loss: 0.4131 - accuracy: 0.8467 - val_loss: 0.4977 - val_accuracy: 0.7818\n",
            "Epoch 11/50\n",
            "9/9 - 0s - loss: 0.3924 - accuracy: 0.8540 - val_loss: 0.4821 - val_accuracy: 0.7855\n",
            "Epoch 12/50\n",
            "9/9 - 0s - loss: 0.3729 - accuracy: 0.8577 - val_loss: 0.4679 - val_accuracy: 0.7855\n",
            "Epoch 13/50\n",
            "9/9 - 0s - loss: 0.3563 - accuracy: 0.8613 - val_loss: 0.4547 - val_accuracy: 0.7855\n",
            "Epoch 14/50\n",
            "9/9 - 0s - loss: 0.3392 - accuracy: 0.8686 - val_loss: 0.4419 - val_accuracy: 0.7855\n",
            "Epoch 15/50\n",
            "9/9 - 0s - loss: 0.3222 - accuracy: 0.8650 - val_loss: 0.4311 - val_accuracy: 0.7855\n",
            "Epoch 16/50\n",
            "9/9 - 0s - loss: 0.3076 - accuracy: 0.8650 - val_loss: 0.4192 - val_accuracy: 0.7891\n",
            "Epoch 17/50\n",
            "9/9 - 0s - loss: 0.2937 - accuracy: 0.8723 - val_loss: 0.4094 - val_accuracy: 0.7964\n",
            "Epoch 18/50\n",
            "9/9 - 0s - loss: 0.2808 - accuracy: 0.8759 - val_loss: 0.4009 - val_accuracy: 0.8000\n",
            "Epoch 19/50\n",
            "9/9 - 0s - loss: 0.2682 - accuracy: 0.8832 - val_loss: 0.3941 - val_accuracy: 0.8000\n",
            "Epoch 20/50\n",
            "9/9 - 0s - loss: 0.2565 - accuracy: 0.8905 - val_loss: 0.3848 - val_accuracy: 0.8073\n",
            "Epoch 21/50\n",
            "9/9 - 0s - loss: 0.2452 - accuracy: 0.8942 - val_loss: 0.3780 - val_accuracy: 0.8109\n",
            "Epoch 22/50\n",
            "9/9 - 0s - loss: 0.2354 - accuracy: 0.9051 - val_loss: 0.3710 - val_accuracy: 0.8182\n",
            "Epoch 23/50\n",
            "9/9 - 0s - loss: 0.2259 - accuracy: 0.9088 - val_loss: 0.3663 - val_accuracy: 0.8218\n",
            "Epoch 24/50\n",
            "9/9 - 0s - loss: 0.2167 - accuracy: 0.9161 - val_loss: 0.3613 - val_accuracy: 0.8255\n",
            "Epoch 25/50\n",
            "9/9 - 0s - loss: 0.2085 - accuracy: 0.9124 - val_loss: 0.3554 - val_accuracy: 0.8327\n",
            "Epoch 26/50\n",
            "9/9 - 0s - loss: 0.2012 - accuracy: 0.9161 - val_loss: 0.3517 - val_accuracy: 0.8291\n",
            "Epoch 27/50\n",
            "9/9 - 0s - loss: 0.1940 - accuracy: 0.9234 - val_loss: 0.3489 - val_accuracy: 0.8364\n",
            "Epoch 28/50\n",
            "9/9 - 0s - loss: 0.1874 - accuracy: 0.9234 - val_loss: 0.3447 - val_accuracy: 0.8436\n",
            "Epoch 29/50\n",
            "9/9 - 0s - loss: 0.1794 - accuracy: 0.9380 - val_loss: 0.3411 - val_accuracy: 0.8509\n",
            "Epoch 30/50\n",
            "9/9 - 0s - loss: 0.1722 - accuracy: 0.9380 - val_loss: 0.3369 - val_accuracy: 0.8582\n",
            "Epoch 31/50\n",
            "9/9 - 0s - loss: 0.1652 - accuracy: 0.9380 - val_loss: 0.3323 - val_accuracy: 0.8655\n",
            "Epoch 32/50\n",
            "9/9 - 0s - loss: 0.1584 - accuracy: 0.9380 - val_loss: 0.3311 - val_accuracy: 0.8545\n",
            "Epoch 33/50\n",
            "9/9 - 0s - loss: 0.1512 - accuracy: 0.9380 - val_loss: 0.3278 - val_accuracy: 0.8691\n",
            "Epoch 34/50\n",
            "9/9 - 0s - loss: 0.1454 - accuracy: 0.9380 - val_loss: 0.3282 - val_accuracy: 0.8509\n",
            "Epoch 35/50\n",
            "9/9 - 0s - loss: 0.1402 - accuracy: 0.9489 - val_loss: 0.3272 - val_accuracy: 0.8545\n",
            "Epoch 36/50\n",
            "9/9 - 0s - loss: 0.1350 - accuracy: 0.9599 - val_loss: 0.3257 - val_accuracy: 0.8582\n",
            "Epoch 37/50\n",
            "9/9 - 0s - loss: 0.1289 - accuracy: 0.9599 - val_loss: 0.3251 - val_accuracy: 0.8655\n",
            "Epoch 38/50\n",
            "9/9 - 0s - loss: 0.1244 - accuracy: 0.9635 - val_loss: 0.3250 - val_accuracy: 0.8655\n",
            "Epoch 39/50\n",
            "9/9 - 0s - loss: 0.1201 - accuracy: 0.9635 - val_loss: 0.3237 - val_accuracy: 0.8836\n",
            "Epoch 40/50\n",
            "9/9 - 0s - loss: 0.1158 - accuracy: 0.9635 - val_loss: 0.3230 - val_accuracy: 0.8800\n",
            "Epoch 41/50\n",
            "9/9 - 0s - loss: 0.1122 - accuracy: 0.9672 - val_loss: 0.3217 - val_accuracy: 0.8836\n",
            "Epoch 42/50\n",
            "9/9 - 0s - loss: 0.1080 - accuracy: 0.9708 - val_loss: 0.3215 - val_accuracy: 0.8836\n",
            "Epoch 43/50\n",
            "9/9 - 0s - loss: 0.1040 - accuracy: 0.9635 - val_loss: 0.3200 - val_accuracy: 0.8800\n",
            "Epoch 44/50\n",
            "9/9 - 0s - loss: 0.0996 - accuracy: 0.9672 - val_loss: 0.3210 - val_accuracy: 0.8800\n",
            "Epoch 45/50\n",
            "9/9 - 0s - loss: 0.0959 - accuracy: 0.9672 - val_loss: 0.3223 - val_accuracy: 0.8800\n",
            "Epoch 46/50\n",
            "9/9 - 0s - loss: 0.0927 - accuracy: 0.9818 - val_loss: 0.3228 - val_accuracy: 0.8800\n",
            "Epoch 47/50\n",
            "9/9 - 0s - loss: 0.0898 - accuracy: 0.9781 - val_loss: 0.3232 - val_accuracy: 0.8764\n",
            "Epoch 48/50\n",
            "9/9 - 0s - loss: 0.0879 - accuracy: 0.9708 - val_loss: 0.3249 - val_accuracy: 0.8764\n",
            "Epoch 49/50\n",
            "9/9 - 0s - loss: 0.0841 - accuracy: 0.9818 - val_loss: 0.3230 - val_accuracy: 0.8836\n",
            "Epoch 50/50\n",
            "9/9 - 0s - loss: 0.0831 - accuracy: 0.9781 - val_loss: 0.3244 - val_accuracy: 0.8836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rYkzjY0Jpn4",
        "outputId": "850d5861-cbbe-4c4f-b2bb-1cf1d48106c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "tf.keras.utils.plot_model(nn, show_shapes=True, rankdir=\"LR\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw6xMQP-O5-X"
      },
      "source": [
        "###***Deep Learning Neural Netwrok Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-6Z3XUQsali",
        "outputId": "bc2f770f-f89a-4426-ce09-281b3d062413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=1)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 0s 2ms/step - loss: 0.7198 - accuracy: 0.7814\n",
            "Loss: 0.7197991609573364, Accuracy: 0.7814207673072815\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uQLqejXO5-Z"
      },
      "source": [
        "# # Evaluate the model using the test data\n",
        "# model_loss, FalseNegatives,FalsePositives, TrueNegatives, TruePositives, Precision, Recall = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "# print(f\"\"\"\\t  Loss: {model_loss},\n",
        "#           FalseNegatives: {FalseNegatives},\n",
        "#           FalsePositives: {FalsePositives},\n",
        "#           TrueNegatives: {TrueNegatives},\n",
        "#           TruePositives: {TruePositives},\n",
        "#           Precision: {Precision},\n",
        "#           Recall: {Recall},\"\"\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qdKZ89DiBfB",
        "outputId": "0dadf850-da0e-442d-a195-fba96f006dc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predictions = nn.predict(X_test_scaled)\n",
        "adjusted_predictions = [(100 * x[0]) for x in predictions]\n",
        "adjusted_predictions"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.04712939262390137,\n",
              " 0.012674927711486816,\n",
              " 70.4321563243866,\n",
              " 0.03386735916137695,\n",
              " 0.025317072868347168,\n",
              " 99.9564528465271,\n",
              " 93.50011348724365,\n",
              " 78.21519374847412,\n",
              " 47.12570905685425,\n",
              " 3.212273120880127,\n",
              " 1.0558724403381348,\n",
              " 2.561071515083313,\n",
              " 99.51765537261963,\n",
              " 31.097519397735596,\n",
              " 4.387331008911133,\n",
              " 0.00182277708518086,\n",
              " 95.67495584487915,\n",
              " 70.01405358314514,\n",
              " 1.6955286264419556,\n",
              " 99.63381290435791,\n",
              " 45.53024172782898,\n",
              " 97.28701114654541,\n",
              " 30.25873899459839,\n",
              " 1.027458906173706,\n",
              " 89.86733555793762,\n",
              " 76.6332745552063,\n",
              " 99.95849132537842,\n",
              " 67.73108243942261,\n",
              " 76.83894634246826,\n",
              " 0.33435821533203125,\n",
              " 0.2623438835144043,\n",
              " 99.98420476913452,\n",
              " 0.14771819114685059,\n",
              " 18.428605794906616,\n",
              " 58.11970829963684,\n",
              " 0.0019430397514952347,\n",
              " 98.98737668991089,\n",
              " 99.93563890457153,\n",
              " 99.99493360519409,\n",
              " 0.4988372325897217,\n",
              " 0.0288546085357666,\n",
              " 0.024265050888061523,\n",
              " 38.115620613098145,\n",
              " 39.141568541526794,\n",
              " 1.5599936246871948,\n",
              " 99.96805191040039,\n",
              " 0.08112192153930664,\n",
              " 41.49766564369202,\n",
              " 61.91322207450867,\n",
              " 99.76552724838257,\n",
              " 0.1270294189453125,\n",
              " 93.88967752456665,\n",
              " 99.33648109436035,\n",
              " 7.572376728057861,\n",
              " 99.85496997833252,\n",
              " 0.0517427921295166,\n",
              " 98.19964170455933,\n",
              " 3.641882538795471,\n",
              " 99.98126029968262,\n",
              " 98.44015836715698,\n",
              " 99.83068108558655,\n",
              " 0.629425048828125,\n",
              " 97.45036959648132,\n",
              " 0.09942352771759033,\n",
              " 90.57081937789917,\n",
              " 31.731805205345154,\n",
              " 50.57252049446106,\n",
              " 98.68701696395874,\n",
              " 99.22232627868652,\n",
              " 57.40072727203369,\n",
              " 5.226495862007141,\n",
              " 99.96955394744873,\n",
              " 0.2255767583847046,\n",
              " 85.85286140441895,\n",
              " 0.054016709327697754,\n",
              " 95.28279304504395,\n",
              " 0.08578896522521973,\n",
              " 99.3302047252655,\n",
              " 93.57328414916992,\n",
              " 99.50835704803467,\n",
              " 4.38251793384552,\n",
              " 0.24374425411224365,\n",
              " 0.9672284126281738,\n",
              " 99.89059567451477,\n",
              " 99.28529262542725,\n",
              " 0.04730522632598877,\n",
              " 0.08036196231842041,\n",
              " 8.162859082221985,\n",
              " 0.5647659301757812,\n",
              " 89.7871196269989,\n",
              " 99.78340864181519,\n",
              " 0.10814368724822998,\n",
              " 0.018721818923950195,\n",
              " 98.56375455856323,\n",
              " 15.505102276802063,\n",
              " 29.25405502319336,\n",
              " 95.05440592765808,\n",
              " 0.09088218212127686,\n",
              " 99.97080564498901,\n",
              " 99.86534118652344,\n",
              " 0.038057565689086914,\n",
              " 0.08612275123596191,\n",
              " 94.68103647232056,\n",
              " 68.40694546699524,\n",
              " 99.73134994506836,\n",
              " 77.45354175567627,\n",
              " 56.40139579772949,\n",
              " 15.180635452270508,\n",
              " 95.46605348587036,\n",
              " 3.477621078491211,\n",
              " 10.15474796295166,\n",
              " 0.09888112545013428,\n",
              " 99.50666427612305,\n",
              " 20.684000849723816,\n",
              " 99.96678829193115,\n",
              " 1.7138123512268066,\n",
              " 1.9993215799331665,\n",
              " 99.59689378738403,\n",
              " 0.005229224188951775,\n",
              " 1.6089767217636108,\n",
              " 5.688449740409851,\n",
              " 46.712473034858704,\n",
              " 18.33031177520752,\n",
              " 0.06948411464691162,\n",
              " 99.98782277107239,\n",
              " 0.11563599109649658,\n",
              " 21.025803685188293,\n",
              " 0.22658109664916992,\n",
              " 1.1249810457229614,\n",
              " 64.70718383789062,\n",
              " 97.67012596130371,\n",
              " 99.84166622161865,\n",
              " 99.97820258140564,\n",
              " 0.32448768615722656,\n",
              " 99.91711378097534,\n",
              " 67.41466522216797,\n",
              " 99.88462924957275,\n",
              " 0.18960237503051758,\n",
              " 44.0483033657074,\n",
              " 0.004849774268222973,\n",
              " 71.13677859306335,\n",
              " 99.77262020111084,\n",
              " 99.88938570022583,\n",
              " 0.04749596118927002,\n",
              " 19.667714834213257,\n",
              " 99.07466173171997,\n",
              " 99.82386827468872,\n",
              " 43.48920285701752,\n",
              " 90.47056436538696,\n",
              " 1.0648667812347412,\n",
              " 98.83995652198792,\n",
              " 0.540924072265625,\n",
              " 0.0029558925234596245,\n",
              " 4.627937078475952,\n",
              " 0.01398921012878418,\n",
              " 0.35492777824401855,\n",
              " 16.491082310676575,\n",
              " 0.9466439485549927,\n",
              " 0.4028201103210449,\n",
              " 64.12742137908936,\n",
              " 0.7412165403366089,\n",
              " 31.050294637680054,\n",
              " 83.19998979568481,\n",
              " 1.5776097774505615,\n",
              " 99.94776248931885,\n",
              " 99.7417688369751,\n",
              " 11.960446834564209,\n",
              " 41.50657653808594,\n",
              " 54.91410493850708,\n",
              " 0.022351741790771484,\n",
              " 80.71660995483398,\n",
              " 2.8224200010299683,\n",
              " 99.71665143966675,\n",
              " 0.7064640522003174,\n",
              " 16.44441783428192,\n",
              " 0.8166909217834473,\n",
              " 31.788495182991028,\n",
              " 97.64253497123718,\n",
              " 72.03779816627502,\n",
              " 1.031753420829773,\n",
              " 99.83773231506348,\n",
              " 89.38242197036743,\n",
              " 0.05033314228057861]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_6G4L1TO5-d"
      },
      "source": [
        "### ***Random Forest Classifier Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stnp4Ep3O5-d",
        "outputId": "d0306623-1e79-4397-fcb1-9bbd90682228",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create a random forest classifier.\n",
        "rf_model = RandomForestClassifier(n_estimators=256, random_state=32)\n",
        "\n",
        "# Fitting the model\n",
        "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = rf_model.predict(X_test_scaled)\n",
        "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Random forest predictive accuracy: 1.000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_pG1RM0JDwr",
        "outputId": "5980f201-80eb-43a1-9c8b-1d928f40b33b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Confusion matrix\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "print(matrix)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 75   0]\n",
            " [  0 108]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkIB9d0lslob"
      },
      "source": [
        "### ***Logistic Regression Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baU-hliossZ1"
      },
      "source": [
        "# Create a Logistic Regression Model\n",
        "classifier = LogisticRegression(solver='lbfgs',\n",
        "                                max_iter=200,\n",
        "                                random_state=1)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAiKwCp4suyg",
        "outputId": "a99b58ed-65e6-402c-e76d-c60689965cd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Fit (train) or model using the training data\n",
        "classifier.fit(X_train_scaled, y_train)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tiOZe0KQtHm1",
        "outputId": "63d50a25-12c2-4c1a-d0fb-07b8a6c8e59c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "# Make predictions\n",
        "y_pred = classifier.predict(X_test_scaled)\n",
        "results = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
        "results.head(20)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Prediction  Actual\n",
              "0            0       0\n",
              "1            0       0\n",
              "2            1       1\n",
              "3            0       0\n",
              "4            0       0\n",
              "5            1       1\n",
              "6            1       1\n",
              "7            1       1\n",
              "8            1       1\n",
              "9            1       1\n",
              "10           0       0\n",
              "11           0       0\n",
              "12           1       1\n",
              "13           0       0\n",
              "14           0       0\n",
              "15           0       0\n",
              "16           1       0\n",
              "17           1       1\n",
              "18           1       1\n",
              "19           1       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUKD1EJUtLhy",
        "outputId": "cbd23322-2c23-4be9-e43c-da71534b768e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.907103825136612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dp_KzZnUJQg_",
        "outputId": "07aa8593-bf30-4809-b41a-ddaa19a7f3ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Confusion matrix\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "print(matrix)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[68  7]\n",
            " [10 98]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QIUMu-a8ZDw"
      },
      "source": [
        "## ***Wine & Weather - Drop All soil columns***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fl3c8lXV8ZDx"
      },
      "source": [
        "# Split our preprocessed data into our features and target arrays\n",
        "y = White_Soil_ML_df[\"quality\"].values\n",
        "X = White_Soil_ML_df.drop([\"quality\", \"is_primeurs\", \"journalist_count\", \"confidence_index_A\",\"confidence_index_A+\", \"confidence_index_B\", \"confidence_index_B+\",\"confidence_index_C\", \"confidence_index_C+\",'avgPrcpFebruary',\n",
        " 'bdod_0-100cm',\n",
        " 'bdod_100-200cm',\n",
        " 'cec_0-100cm',\n",
        " 'cec_100-200cm',\n",
        " 'cfvo_0-100cm',\n",
        " 'cfvo_100-200cm',\n",
        " 'clay_0-100cm',\n",
        " 'clay_100-200cm',\n",
        " 'nitrogen_0-100cm',\n",
        " 'nitrogen_100-200cm',\n",
        " 'ocd_0-100cm',\n",
        " 'ocd_100-200cm',\n",
        " 'ocs_0-30cm',\n",
        " 'phh2o_0-100cm',\n",
        " 'phh2o_100-200cm',\n",
        " 'sand_0-100cm',\n",
        " 'sand_100-200cm',\n",
        " 'silt_0-100cm',\n",
        " 'silt_100-200cm',\n",
        " 'soc_0-100cm',\n",
        " 'soc_100-200cm',],1).values"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgW_GLNG8ZD1"
      },
      "source": [
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X)\n",
        "X_scaled = X_scaler.transform(X)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naWidjg3K2Be"
      },
      "source": [
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, random_state=45)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDkDyjKx8ZD4"
      },
      "source": [
        "###***Deep Learning Neural Netwrok for training data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lbMo7m578ZD4",
        "outputId": "6919b045-0268-460a-a0ac-fbf33719bba0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define the model - deep neural net\n",
        "number_input_features = len(X_train_scaled[-1])\n",
        "hidden_nodes_layer1 =  24\n",
        "hidden_nodes_layer2 = 12\n",
        "\n",
        "# metrics = [\n",
        "#     tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
        "#     tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
        "#     tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
        "#     tf.keras.metrics.TruePositives(name=\"tp\"),\n",
        "#     tf.keras.metrics.Precision(name=\"precision\"),\n",
        "#     tf.keras.metrics.Recall(name=\"recall\"),\n",
        "# ]\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile the Sequential model together and customize metrics\n",
        "nn.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train_scaled, y_train, epochs=50, validation_split=0.5)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.9655 - accuracy: 0.4745 - val_loss: 0.8514 - val_accuracy: 0.5200\n",
            "Epoch 2/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.8536 - accuracy: 0.4818 - val_loss: 0.7931 - val_accuracy: 0.5273\n",
            "Epoch 3/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.7727 - accuracy: 0.5109 - val_loss: 0.7518 - val_accuracy: 0.5309\n",
            "Epoch 4/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.7205 - accuracy: 0.5511 - val_loss: 0.7198 - val_accuracy: 0.5782\n",
            "Epoch 5/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.6767 - accuracy: 0.6241 - val_loss: 0.6937 - val_accuracy: 0.6036\n",
            "Epoch 6/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.6461 - accuracy: 0.6642 - val_loss: 0.6704 - val_accuracy: 0.6291\n",
            "Epoch 7/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.6158 - accuracy: 0.6898 - val_loss: 0.6497 - val_accuracy: 0.6509\n",
            "Epoch 8/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.5903 - accuracy: 0.7372 - val_loss: 0.6312 - val_accuracy: 0.6618\n",
            "Epoch 9/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.5670 - accuracy: 0.7445 - val_loss: 0.6122 - val_accuracy: 0.6873\n",
            "Epoch 10/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.5444 - accuracy: 0.7628 - val_loss: 0.5942 - val_accuracy: 0.7018\n",
            "Epoch 11/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.5228 - accuracy: 0.7993 - val_loss: 0.5780 - val_accuracy: 0.7164\n",
            "Epoch 12/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.5017 - accuracy: 0.8102 - val_loss: 0.5614 - val_accuracy: 0.7345\n",
            "Epoch 13/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4798 - accuracy: 0.8212 - val_loss: 0.5444 - val_accuracy: 0.7455\n",
            "Epoch 14/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4585 - accuracy: 0.8248 - val_loss: 0.5277 - val_accuracy: 0.7564\n",
            "Epoch 15/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4370 - accuracy: 0.8431 - val_loss: 0.5108 - val_accuracy: 0.7709\n",
            "Epoch 16/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4151 - accuracy: 0.8540 - val_loss: 0.4938 - val_accuracy: 0.7782\n",
            "Epoch 17/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.3933 - accuracy: 0.8832 - val_loss: 0.4767 - val_accuracy: 0.7964\n",
            "Epoch 18/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3709 - accuracy: 0.8832 - val_loss: 0.4593 - val_accuracy: 0.8073\n",
            "Epoch 19/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3497 - accuracy: 0.8759 - val_loss: 0.4442 - val_accuracy: 0.8109\n",
            "Epoch 20/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3276 - accuracy: 0.8796 - val_loss: 0.4282 - val_accuracy: 0.8145\n",
            "Epoch 21/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.3071 - accuracy: 0.8832 - val_loss: 0.4140 - val_accuracy: 0.8145\n",
            "Epoch 22/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2874 - accuracy: 0.8869 - val_loss: 0.3989 - val_accuracy: 0.8109\n",
            "Epoch 23/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2693 - accuracy: 0.8942 - val_loss: 0.3864 - val_accuracy: 0.8182\n",
            "Epoch 24/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2523 - accuracy: 0.9088 - val_loss: 0.3761 - val_accuracy: 0.8218\n",
            "Epoch 25/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2378 - accuracy: 0.9161 - val_loss: 0.3674 - val_accuracy: 0.8291\n",
            "Epoch 26/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2221 - accuracy: 0.9197 - val_loss: 0.3600 - val_accuracy: 0.8364\n",
            "Epoch 27/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2085 - accuracy: 0.9197 - val_loss: 0.3528 - val_accuracy: 0.8473\n",
            "Epoch 28/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1956 - accuracy: 0.9234 - val_loss: 0.3464 - val_accuracy: 0.8582\n",
            "Epoch 29/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1844 - accuracy: 0.9416 - val_loss: 0.3389 - val_accuracy: 0.8618\n",
            "Epoch 30/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1720 - accuracy: 0.9489 - val_loss: 0.3339 - val_accuracy: 0.8582\n",
            "Epoch 31/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1635 - accuracy: 0.9380 - val_loss: 0.3296 - val_accuracy: 0.8655\n",
            "Epoch 32/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1532 - accuracy: 0.9489 - val_loss: 0.3290 - val_accuracy: 0.8655\n",
            "Epoch 33/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1444 - accuracy: 0.9599 - val_loss: 0.3238 - val_accuracy: 0.8727\n",
            "Epoch 34/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1377 - accuracy: 0.9562 - val_loss: 0.3189 - val_accuracy: 0.8836\n",
            "Epoch 35/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1284 - accuracy: 0.9708 - val_loss: 0.3165 - val_accuracy: 0.8800\n",
            "Epoch 36/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1218 - accuracy: 0.9781 - val_loss: 0.3166 - val_accuracy: 0.8836\n",
            "Epoch 37/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1150 - accuracy: 0.9781 - val_loss: 0.3094 - val_accuracy: 0.8982\n",
            "Epoch 38/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1081 - accuracy: 0.9854 - val_loss: 0.3063 - val_accuracy: 0.8982\n",
            "Epoch 39/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1029 - accuracy: 0.9818 - val_loss: 0.3037 - val_accuracy: 0.8945\n",
            "Epoch 40/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0978 - accuracy: 0.9854 - val_loss: 0.3043 - val_accuracy: 0.8909\n",
            "Epoch 41/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0913 - accuracy: 0.9854 - val_loss: 0.3012 - val_accuracy: 0.8945\n",
            "Epoch 42/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0868 - accuracy: 0.9854 - val_loss: 0.2989 - val_accuracy: 0.9018\n",
            "Epoch 43/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0828 - accuracy: 0.9891 - val_loss: 0.2965 - val_accuracy: 0.9018\n",
            "Epoch 44/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0787 - accuracy: 0.9927 - val_loss: 0.2995 - val_accuracy: 0.8909\n",
            "Epoch 45/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0742 - accuracy: 0.9927 - val_loss: 0.2939 - val_accuracy: 0.8982\n",
            "Epoch 46/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0709 - accuracy: 0.9927 - val_loss: 0.2927 - val_accuracy: 0.9055\n",
            "Epoch 47/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0667 - accuracy: 0.9927 - val_loss: 0.2995 - val_accuracy: 0.8982\n",
            "Epoch 48/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0644 - accuracy: 0.9964 - val_loss: 0.2950 - val_accuracy: 0.8945\n",
            "Epoch 49/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0616 - accuracy: 0.9964 - val_loss: 0.2911 - val_accuracy: 0.9018\n",
            "Epoch 50/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0578 - accuracy: 0.9964 - val_loss: 0.2923 - val_accuracy: 0.8982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGyzl5ZaMB8L",
        "outputId": "39f26f7c-8e4e-4f3d-bc3c-98e255ca05de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "tf.keras.utils.plot_model(nn, show_shapes=True, rankdir=\"LR\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEcRZGXM8ZD8"
      },
      "source": [
        "###***Deep Learning Neural Netwrok Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2_bXNR4tFKY",
        "outputId": "d425b6c8-d58e-417d-8ee9-01cc2b36eaef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6/6 - 0s - loss: 0.5675 - accuracy: 0.8087\n",
            "Loss: 0.5675416588783264, Accuracy: 0.8087431788444519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRFv96XA8ZD8"
      },
      "source": [
        "# # Evaluate the model using the test data\n",
        "# model_loss, FalseNegatives,FalsePositives, TrueNegatives, TruePositives, Precision, Recall = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "# print(f\"\"\"\\t  Loss: {model_loss},\n",
        "#           FalseNegatives: {FalseNegatives},\n",
        "#           FalsePositives: {FalsePositives},\n",
        "#           TrueNegatives: {TrueNegatives},\n",
        "#           TruePositives: {TruePositives},\n",
        "#           Precision: {Precision},\n",
        "#           Recall: {Recall},\"\"\")\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNJMznkl8ZEB",
        "outputId": "496360a1-a8c5-460c-c494-77f0246e80e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predictions = nn.predict(X_test_scaled)\n",
        "adjusted_predictions = [(100 * x[0]) for x in predictions]\n",
        "adjusted_predictions"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.11618435382843018,\n",
              " 0.5198776721954346,\n",
              " 62.0038628578186,\n",
              " 0.3958165645599365,\n",
              " 0.3394603729248047,\n",
              " 78.36841344833374,\n",
              " 96.1423933506012,\n",
              " 41.588592529296875,\n",
              " 67.20591187477112,\n",
              " 27.9000461101532,\n",
              " 0.2155900001525879,\n",
              " 5.819091200828552,\n",
              " 94.52885389328003,\n",
              " 21.436306834220886,\n",
              " 12.412065267562866,\n",
              " 0.0559687614440918,\n",
              " 95.2720046043396,\n",
              " 93.27788352966309,\n",
              " 13.13711702823639,\n",
              " 96.14452719688416,\n",
              " 34.888237714767456,\n",
              " 98.42599630355835,\n",
              " 91.8314516544342,\n",
              " 2.2496670484542847,\n",
              " 97.65373468399048,\n",
              " 51.49503946304321,\n",
              " 99.8794436454773,\n",
              " 91.35390520095825,\n",
              " 95.26187181472778,\n",
              " 0.5893737077713013,\n",
              " 1.3247370719909668,\n",
              " 99.55573081970215,\n",
              " 0.3311455249786377,\n",
              " 11.238867044448853,\n",
              " 33.03361535072327,\n",
              " 1.061481237411499,\n",
              " 99.78134036064148,\n",
              " 99.99364614486694,\n",
              " 99.95635151863098,\n",
              " 0.9511381387710571,\n",
              " 10.047289729118347,\n",
              " 0.29473602771759033,\n",
              " 17.87199378013611,\n",
              " 39.968353509902954,\n",
              " 12.2128427028656,\n",
              " 99.99318718910217,\n",
              " 3.794512152671814,\n",
              " 35.7992947101593,\n",
              " 19.015294313430786,\n",
              " 99.95715618133545,\n",
              " 8.373311161994934,\n",
              " 93.87099146842957,\n",
              " 99.84307289123535,\n",
              " 8.91411304473877,\n",
              " 99.94887113571167,\n",
              " 0.7677078247070312,\n",
              " 98.97441864013672,\n",
              " 1.7050832509994507,\n",
              " 99.93869066238403,\n",
              " 99.38705563545227,\n",
              " 93.45471858978271,\n",
              " 1.7942458391189575,\n",
              " 96.81390523910522,\n",
              " 0.7029503583908081,\n",
              " 68.0578351020813,\n",
              " 97.13261723518372,\n",
              " 47.660067677497864,\n",
              " 95.29039859771729,\n",
              " 99.9632477760315,\n",
              " 94.32337284088135,\n",
              " 83.35961103439331,\n",
              " 99.99738931655884,\n",
              " 11.113128066062927,\n",
              " 98.71339797973633,\n",
              " 6.0430169105529785,\n",
              " 97.29837775230408,\n",
              " 0.8523851633071899,\n",
              " 93.65639686584473,\n",
              " 86.33621335029602,\n",
              " 99.79245066642761,\n",
              " 33.97446870803833,\n",
              " 0.5157381296157837,\n",
              " 1.4759272336959839,\n",
              " 97.03460931777954,\n",
              " 99.80018138885498,\n",
              " 0.18781721591949463,\n",
              " 0.13985633850097656,\n",
              " 73.89299869537354,\n",
              " 3.1067967414855957,\n",
              " 99.94690418243408,\n",
              " 96.41661643981934,\n",
              " 0.5639463663101196,\n",
              " 0.04696547985076904,\n",
              " 99.4026780128479,\n",
              " 8.855107426643372,\n",
              " 1.797240972518921,\n",
              " 99.80593919754028,\n",
              " 0.8606821298599243,\n",
              " 99.97312426567078,\n",
              " 99.54766035079956,\n",
              " 0.028693675994873047,\n",
              " 97.92977571487427,\n",
              " 98.61409664154053,\n",
              " 57.809650897979736,\n",
              " 99.98297095298767,\n",
              " 78.63428592681885,\n",
              " 87.61916160583496,\n",
              " 7.069268822669983,\n",
              " 97.50114679336548,\n",
              " 18.331989645957947,\n",
              " 13.372600078582764,\n",
              " 1.2265115976333618,\n",
              " 99.24383163452148,\n",
              " 26.973527669906616,\n",
              " 99.86658096313477,\n",
              " 6.084910035133362,\n",
              " 3.862622380256653,\n",
              " 97.2684383392334,\n",
              " 0.07834732532501221,\n",
              " 2.8488129377365112,\n",
              " 10.62319278717041,\n",
              " 14.742571115493774,\n",
              " 94.94242668151855,\n",
              " 4.009637236595154,\n",
              " 99.99152421951294,\n",
              " 0.7066071033477783,\n",
              " 25.297898054122925,\n",
              " 0.5224376916885376,\n",
              " 4.4702112674713135,\n",
              " 91.97393655776978,\n",
              " 99.61545467376709,\n",
              " 99.92042779922485,\n",
              " 99.471515417099,\n",
              " 0.8276820182800293,\n",
              " 99.96974468231201,\n",
              " 61.29711866378784,\n",
              " 99.97483491897583,\n",
              " 1.416999101638794,\n",
              " 68.22314262390137,\n",
              " 0.022175908088684082,\n",
              " 94.56440806388855,\n",
              " 99.9087929725647,\n",
              " 99.98014569282532,\n",
              " 0.6272882223129272,\n",
              " 21.181803941726685,\n",
              " 99.78951215744019,\n",
              " 97.28114008903503,\n",
              " 77.1708607673645,\n",
              " 94.78732347488403,\n",
              " 0.8567184209823608,\n",
              " 92.45758652687073,\n",
              " 2.5632858276367188,\n",
              " 0.2642601728439331,\n",
              " 2.762293815612793,\n",
              " 0.06051361560821533,\n",
              " 0.4439830780029297,\n",
              " 19.315823912620544,\n",
              " 0.13355910778045654,\n",
              " 3.425225615501404,\n",
              " 89.18228149414062,\n",
              " 2.839970588684082,\n",
              " 12.211531400680542,\n",
              " 93.65867972373962,\n",
              " 1.4257222414016724,\n",
              " 99.96734857559204,\n",
              " 99.90362524986267,\n",
              " 39.44055438041687,\n",
              " 19.516649842262268,\n",
              " 26.231759786605835,\n",
              " 0.5607962608337402,\n",
              " 84.0782880783081,\n",
              " 4.68038022518158,\n",
              " 99.54771995544434,\n",
              " 0.4562288522720337,\n",
              " 84.89398956298828,\n",
              " 1.5391767024993896,\n",
              " 5.241575837135315,\n",
              " 99.54928159713745,\n",
              " 94.65761184692383,\n",
              " 3.788718581199646,\n",
              " 99.98463988304138,\n",
              " 95.58570981025696,\n",
              " 0.08299946784973145]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B21n32Y_8ZEI"
      },
      "source": [
        "### ***Random Forest Classifier Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbi4Nt6S8ZEI",
        "outputId": "84cfed88-7bd3-4bbf-9e03-dcd3c55a44d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create a random forest classifier.\n",
        "rf_model = RandomForestClassifier(n_estimators=256, random_state=32)\n",
        "\n",
        "# Fitting the model\n",
        "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = rf_model.predict(X_test_scaled)\n",
        "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Random forest predictive accuracy: 0.956\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ICcqS8oLBMd",
        "outputId": "cd20c1f5-7ca3-4e1f-96b4-0eb1a05a9c47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Confusion matrix\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "print(matrix)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 72   3]\n",
            " [  5 103]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NPfSMqy8ZEO"
      },
      "source": [
        "### ***Logistic Regression Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkpcmGoI8ZEP"
      },
      "source": [
        "# Create a Logistic Regression Model\n",
        "classifier = LogisticRegression(solver='lbfgs',\n",
        "                                max_iter=200,\n",
        "                                random_state=1)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rh5RRLdU8ZEQ",
        "outputId": "7197013f-5bdd-49aa-de7f-6b552e4825bc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Fit (train) or model using the training data\n",
        "classifier.fit(X_train_scaled, y_train)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBrQHR2e8ZER",
        "outputId": "0a4fbe1e-b510-4ee6-bb06-79cc2b726f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "# Make predictions\n",
        "y_pred = classifier.predict(X_test_scaled)\n",
        "results = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
        "results.head(20)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Prediction  Actual\n",
              "0            0       0\n",
              "1            0       0\n",
              "2            1       1\n",
              "3            0       0\n",
              "4            0       0\n",
              "5            1       1\n",
              "6            1       1\n",
              "7            1       1\n",
              "8            1       1\n",
              "9            1       1\n",
              "10           0       0\n",
              "11           0       0\n",
              "12           1       1\n",
              "13           0       0\n",
              "14           0       0\n",
              "15           0       0\n",
              "16           1       0\n",
              "17           1       1\n",
              "18           1       1\n",
              "19           1       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXcYvmte8ZET",
        "outputId": "229c4d0a-37ab-4778-ac39-8d3788a1fd48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.907103825136612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IgWEnV9LFQH",
        "outputId": "52a68fb6-f95a-4e7a-df1f-c4273285e89e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Confusion matrix\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "print(matrix)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[68  7]\n",
            " [10 98]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnWP0hsq8zrQ"
      },
      "source": [
        "## ***Wine & Soil - Drop All weather columns***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zjTIfIy8zrR"
      },
      "source": [
        "# Split our preprocessed data into our features and target arrays\n",
        "y = White_Soil_ML_df[\"quality\"].values\n",
        "X = White_Soil_ML_df.drop([\"quality\",\"is_primeurs\", \"journalist_count\", \"confidence_index_A\",\"confidence_index_A+\", \"confidence_index_B\", \"confidence_index_B+\",\"confidence_index_C\", \"confidence_index_C+\",'avgPrcpFebruary',\n",
        " 'avgTempFebruary',\n",
        " 'avgPrcpMarch',\n",
        " 'avgTempMarch',\n",
        " 'avgPrcpApril',\n",
        " 'avgTempApril',\n",
        " 'avgPrcpMay',\n",
        " 'avgTempMay',\n",
        " 'avgPrcpJune',\n",
        " 'avgTempJune',\n",
        " 'avgPrcpJuly',\n",
        " 'avgTempJuly',\n",
        " 'avgPrcpAugust',\n",
        " 'avgTempAugust',\n",
        " 'avgPrcpSeptember',\n",
        " 'avgTempSeptember',\n",
        " 'avgPrcpOctober',\n",
        " 'avgTempOctober'],1).values"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvoFDc1r8zrV"
      },
      "source": [
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X)\n",
        "X_scaled = X_scaler.transform(X)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZBWe_JuLLVK"
      },
      "source": [
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, random_state=45)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLZR5S938zrY"
      },
      "source": [
        "###***Deep Learning Neural Netwrok for training data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GH0R3gB8zrZ",
        "outputId": "709618e9-2a9d-4c12-ef14-25652f491080",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define the model - deep neural net\n",
        "number_input_features = len(X_train_scaled[-1])\n",
        "hidden_nodes_layer1 =  24\n",
        "hidden_nodes_layer2 = 12\n",
        "\n",
        "# metrics = [\n",
        "#     tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
        "#     tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
        "#     tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
        "#     tf.keras.metrics.TruePositives(name=\"tp\"),\n",
        "#     tf.keras.metrics.Precision(name=\"precision\"),\n",
        "#     tf.keras.metrics.Recall(name=\"recall\"),\n",
        "# ]\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile the Sequential model together and customize metrics\n",
        "nn.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train_scaled, y_train, epochs=50, validation_split=0.5)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.7315 - accuracy: 0.5292 - val_loss: 0.7084 - val_accuracy: 0.5600\n",
            "Epoch 2/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.6326 - accuracy: 0.6168 - val_loss: 0.6432 - val_accuracy: 0.6764\n",
            "Epoch 3/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.5623 - accuracy: 0.7007 - val_loss: 0.5917 - val_accuracy: 0.7200\n",
            "Epoch 4/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.5081 - accuracy: 0.8029 - val_loss: 0.5496 - val_accuracy: 0.7491\n",
            "Epoch 5/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4622 - accuracy: 0.8321 - val_loss: 0.5123 - val_accuracy: 0.7855\n",
            "Epoch 6/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4257 - accuracy: 0.8467 - val_loss: 0.4856 - val_accuracy: 0.8145\n",
            "Epoch 7/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3952 - accuracy: 0.8613 - val_loss: 0.4613 - val_accuracy: 0.8182\n",
            "Epoch 8/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3686 - accuracy: 0.8650 - val_loss: 0.4436 - val_accuracy: 0.8291\n",
            "Epoch 9/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3449 - accuracy: 0.8832 - val_loss: 0.4260 - val_accuracy: 0.8364\n",
            "Epoch 10/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.3261 - accuracy: 0.8832 - val_loss: 0.4087 - val_accuracy: 0.8291\n",
            "Epoch 11/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3077 - accuracy: 0.8832 - val_loss: 0.3940 - val_accuracy: 0.8327\n",
            "Epoch 12/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2916 - accuracy: 0.8869 - val_loss: 0.3806 - val_accuracy: 0.8400\n",
            "Epoch 13/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2759 - accuracy: 0.8869 - val_loss: 0.3698 - val_accuracy: 0.8400\n",
            "Epoch 14/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2632 - accuracy: 0.9051 - val_loss: 0.3591 - val_accuracy: 0.8473\n",
            "Epoch 15/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2498 - accuracy: 0.9124 - val_loss: 0.3505 - val_accuracy: 0.8436\n",
            "Epoch 16/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2370 - accuracy: 0.9088 - val_loss: 0.3396 - val_accuracy: 0.8509\n",
            "Epoch 17/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2249 - accuracy: 0.9197 - val_loss: 0.3320 - val_accuracy: 0.8545\n",
            "Epoch 18/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.2150 - accuracy: 0.9161 - val_loss: 0.3273 - val_accuracy: 0.8473\n",
            "Epoch 19/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2053 - accuracy: 0.9197 - val_loss: 0.3191 - val_accuracy: 0.8545\n",
            "Epoch 20/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1964 - accuracy: 0.9234 - val_loss: 0.3153 - val_accuracy: 0.8545\n",
            "Epoch 21/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1873 - accuracy: 0.9343 - val_loss: 0.3101 - val_accuracy: 0.8618\n",
            "Epoch 22/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1801 - accuracy: 0.9416 - val_loss: 0.3063 - val_accuracy: 0.8618\n",
            "Epoch 23/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1733 - accuracy: 0.9380 - val_loss: 0.3017 - val_accuracy: 0.8691\n",
            "Epoch 24/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1673 - accuracy: 0.9380 - val_loss: 0.3028 - val_accuracy: 0.8655\n",
            "Epoch 25/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1618 - accuracy: 0.9453 - val_loss: 0.2968 - val_accuracy: 0.8655\n",
            "Epoch 26/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1537 - accuracy: 0.9453 - val_loss: 0.2896 - val_accuracy: 0.8727\n",
            "Epoch 27/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1489 - accuracy: 0.9453 - val_loss: 0.2855 - val_accuracy: 0.8764\n",
            "Epoch 28/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1435 - accuracy: 0.9416 - val_loss: 0.2813 - val_accuracy: 0.8691\n",
            "Epoch 29/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1381 - accuracy: 0.9453 - val_loss: 0.2813 - val_accuracy: 0.8727\n",
            "Epoch 30/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1349 - accuracy: 0.9526 - val_loss: 0.2783 - val_accuracy: 0.8764\n",
            "Epoch 31/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1292 - accuracy: 0.9635 - val_loss: 0.2759 - val_accuracy: 0.8800\n",
            "Epoch 32/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1250 - accuracy: 0.9599 - val_loss: 0.2669 - val_accuracy: 0.8800\n",
            "Epoch 33/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1219 - accuracy: 0.9599 - val_loss: 0.2657 - val_accuracy: 0.8800\n",
            "Epoch 34/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1179 - accuracy: 0.9672 - val_loss: 0.2638 - val_accuracy: 0.8873\n",
            "Epoch 35/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1153 - accuracy: 0.9599 - val_loss: 0.2649 - val_accuracy: 0.8836\n",
            "Epoch 36/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1108 - accuracy: 0.9635 - val_loss: 0.2624 - val_accuracy: 0.8873\n",
            "Epoch 37/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1075 - accuracy: 0.9745 - val_loss: 0.2611 - val_accuracy: 0.8873\n",
            "Epoch 38/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.1040 - accuracy: 0.9672 - val_loss: 0.2586 - val_accuracy: 0.8873\n",
            "Epoch 39/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1022 - accuracy: 0.9672 - val_loss: 0.2565 - val_accuracy: 0.8909\n",
            "Epoch 40/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0997 - accuracy: 0.9708 - val_loss: 0.2542 - val_accuracy: 0.8945\n",
            "Epoch 41/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0960 - accuracy: 0.9745 - val_loss: 0.2533 - val_accuracy: 0.8982\n",
            "Epoch 42/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0933 - accuracy: 0.9818 - val_loss: 0.2508 - val_accuracy: 0.9018\n",
            "Epoch 43/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0917 - accuracy: 0.9708 - val_loss: 0.2499 - val_accuracy: 0.9055\n",
            "Epoch 44/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0901 - accuracy: 0.9708 - val_loss: 0.2494 - val_accuracy: 0.9018\n",
            "Epoch 45/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0876 - accuracy: 0.9781 - val_loss: 0.2477 - val_accuracy: 0.9055\n",
            "Epoch 46/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0864 - accuracy: 0.9708 - val_loss: 0.2461 - val_accuracy: 0.9018\n",
            "Epoch 47/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0836 - accuracy: 0.9745 - val_loss: 0.2474 - val_accuracy: 0.9018\n",
            "Epoch 48/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0808 - accuracy: 0.9818 - val_loss: 0.2491 - val_accuracy: 0.9055\n",
            "Epoch 49/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0849 - accuracy: 0.9745 - val_loss: 0.2493 - val_accuracy: 0.9018\n",
            "Epoch 50/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0795 - accuracy: 0.9781 - val_loss: 0.2426 - val_accuracy: 0.9055\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ag1d7XoMJF9",
        "outputId": "1d72869b-27e8-43fc-d349-2165d1f521d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "tf.keras.utils.plot_model(nn, show_shapes=True, rankdir=\"LR\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-Cju5lo8zrb"
      },
      "source": [
        "###***Deep Learning Neural Network Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZlaGKpUtLqL",
        "outputId": "83014297-787d-4890-ecf5-950d0c588eb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6/6 - 0s - loss: 0.6698 - accuracy: 0.8087\n",
            "Loss: 0.6698299050331116, Accuracy: 0.8087431788444519\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NTSC0Kk8zrc"
      },
      "source": [
        "# # Evaluate the model using the test data\n",
        "# model_loss, FalseNegatives,FalsePositives, TrueNegatives, TruePositives, Precision, Recall = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "# print(f\"\"\"\\t  Loss: {model_loss},\n",
        "#           FalseNegatives: {FalseNegatives},\n",
        "#           FalsePositives: {FalsePositives},\n",
        "#           TrueNegatives: {TrueNegatives},\n",
        "#           TruePositives: {TruePositives},\n",
        "#           Precision: {Precision},\n",
        "#           Recall: {Recall},\"\"\")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFA_Xo9k8zrg",
        "outputId": "c1416839-330f-4730-e987-8f33570a068c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "predictions = nn.predict(X_test_scaled)\n",
        "adjusted_predictions = [(100 * x[0]) for x in predictions]\n",
        "adjusted_predictions"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.13391077518463135,\n",
              " 0.024390220642089844,\n",
              " 50.01722574234009,\n",
              " 0.015997886657714844,\n",
              " 9.775492548942566,\n",
              " 99.98186230659485,\n",
              " 94.61027383804321,\n",
              " 39.815545082092285,\n",
              " 36.224597692489624,\n",
              " 18.60148012638092,\n",
              " 0.04132091999053955,\n",
              " 0.18662214279174805,\n",
              " 96.1532473564148,\n",
              " 20.485809445381165,\n",
              " 6.7156195640563965,\n",
              " 0.0228196382522583,\n",
              " 96.86727523803711,\n",
              " 96.36707305908203,\n",
              " 4.132872819900513,\n",
              " 97.45173454284668,\n",
              " 38.14808130264282,\n",
              " 99.00586605072021,\n",
              " 75.88831782341003,\n",
              " 1.1769235134124756,\n",
              " 95.15756964683533,\n",
              " 80.61195015907288,\n",
              " 99.89286661148071,\n",
              " 73.76173734664917,\n",
              " 78.81055474281311,\n",
              " 0.12441277503967285,\n",
              " 0.07877945899963379,\n",
              " 99.97303485870361,\n",
              " 0.11013150215148926,\n",
              " 17.24960505962372,\n",
              " 17.39385426044464,\n",
              " 0.03007352352142334,\n",
              " 99.82132911682129,\n",
              " 99.94288682937622,\n",
              " 99.9654769897461,\n",
              " 0.3653109073638916,\n",
              " 5.358979105949402,\n",
              " 0.010060261411126703,\n",
              " 25.653815269470215,\n",
              " 68.31676959991455,\n",
              " 3.7036508321762085,\n",
              " 99.59874153137207,\n",
              " 1.0249078273773193,\n",
              " 38.67371380329132,\n",
              " 23.661082983016968,\n",
              " 99.92502927780151,\n",
              " 0.3932654857635498,\n",
              " 95.81170678138733,\n",
              " 99.68963861465454,\n",
              " 5.25783896446228,\n",
              " 98.95772933959961,\n",
              " 0.2505183219909668,\n",
              " 86.55062913894653,\n",
              " 0.4666924476623535,\n",
              " 99.95245933532715,\n",
              " 99.28381443023682,\n",
              " 99.82322454452515,\n",
              " 1.130235195159912,\n",
              " 96.12897634506226,\n",
              " 0.10048747062683105,\n",
              " 75.15141367912292,\n",
              " 35.25891602039337,\n",
              " 43.95793378353119,\n",
              " 98.78117442131042,\n",
              " 99.96085166931152,\n",
              " 90.02329707145691,\n",
              " 56.03126287460327,\n",
              " 99.97128248214722,\n",
              " 4.669976234436035,\n",
              " 71.27358913421631,\n",
              " 0.43489933013916016,\n",
              " 98.24550151824951,\n",
              " 0.10050833225250244,\n",
              " 98.49156141281128,\n",
              " 74.94163513183594,\n",
              " 99.89099502563477,\n",
              " 3.9483457803726196,\n",
              " 0.018402934074401855,\n",
              " 0.09005963802337646,\n",
              " 99.91567134857178,\n",
              " 99.69442486763,\n",
              " 0.010702935833251104,\n",
              " 0.04980266094207764,\n",
              " 0.047466158866882324,\n",
              " 0.8374541997909546,\n",
              " 99.99870657920837,\n",
              " 99.79803562164307,\n",
              " 0.005348098056856543,\n",
              " 0.011750840349122882,\n",
              " 88.30519914627075,\n",
              " 10.697749257087708,\n",
              " 99.17370080947876,\n",
              " 99.96250867843628,\n",
              " 0.08210241794586182,\n",
              " 99.95484352111816,\n",
              " 99.55335855484009,\n",
              " 0.006825184391345829,\n",
              " 99.51373338699341,\n",
              " 97.5171148777008,\n",
              " 68.59170198440552,\n",
              " 99.9224305152893,\n",
              " 66.56750440597534,\n",
              " 87.28715181350708,\n",
              " 11.134558916091919,\n",
              " 98.34878444671631,\n",
              " 1.6548395156860352,\n",
              " 1.0394304990768433,\n",
              " 4.555055499076843,\n",
              " 99.39988851547241,\n",
              " 0.7633328437805176,\n",
              " 99.84327554702759,\n",
              " 0.7354706525802612,\n",
              " 3.2541364431381226,\n",
              " 97.09266424179077,\n",
              " 0.05554556846618652,\n",
              " 2.402523159980774,\n",
              " 0.8862555027008057,\n",
              " 97.96786308288574,\n",
              " 77.28652954101562,\n",
              " 0.7348775863647461,\n",
              " 99.97731447219849,\n",
              " 0.08886456489562988,\n",
              " 19.39951777458191,\n",
              " 0.016891956329345703,\n",
              " 1.326674222946167,\n",
              " 94.02092099189758,\n",
              " 98.92103672027588,\n",
              " 99.799644947052,\n",
              " 99.95925426483154,\n",
              " 0.03533661365509033,\n",
              " 99.88570213317871,\n",
              " 67.3927903175354,\n",
              " 99.79302883148193,\n",
              " 0.1346677541732788,\n",
              " 35.224613547325134,\n",
              " 0.01310110092163086,\n",
              " 71.73693776130676,\n",
              " 99.78605508804321,\n",
              " 99.80882406234741,\n",
              " 0.025218725204467773,\n",
              " 18.347936868667603,\n",
              " 99.4503378868103,\n",
              " 98.45462441444397,\n",
              " 53.37345600128174,\n",
              " 91.01476669311523,\n",
              " 0.2408832311630249,\n",
              " 96.93371057510376,\n",
              " 0.47877728939056396,\n",
              " 0.04598796367645264,\n",
              " 2.9578566551208496,\n",
              " 0.06434917449951172,\n",
              " 65.1584267616272,\n",
              " 31.28320574760437,\n",
              " 0.023165345191955566,\n",
              " 0.3458261489868164,\n",
              " 16.57356023788452,\n",
              " 0.8659422397613525,\n",
              " 96.23767137527466,\n",
              " 87.13250756263733,\n",
              " 2.0160287618637085,\n",
              " 99.90988373756409,\n",
              " 99.40879344940186,\n",
              " 14.334103465080261,\n",
              " 46.06361985206604,\n",
              " 19.7548508644104,\n",
              " 0.19401609897613525,\n",
              " 81.43242597579956,\n",
              " 1.386493444442749,\n",
              " 99.84931945800781,\n",
              " 0.013887882232666016,\n",
              " 66.29582047462463,\n",
              " 0.07671713829040527,\n",
              " 0.0741034746170044,\n",
              " 99.63333010673523,\n",
              " 72.8421688079834,\n",
              " 1.0398805141448975,\n",
              " 98.49275946617126,\n",
              " 95.02729177474976,\n",
              " 0.16394555568695068]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scV4Bm6e8zrn"
      },
      "source": [
        "### ***Random Forest Classifier Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2qj25M98zro",
        "outputId": "8968e51e-d3e3-4a56-c37f-f735b0c473d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create a random forest classifier.\n",
        "rf_model = RandomForestClassifier(n_estimators=256, random_state=32)\n",
        "\n",
        "# Fitting the model\n",
        "rf_model = rf_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Evaluate the model\n",
        "y_pred = rf_model.predict(X_test_scaled)\n",
        "print(f\" Random forest predictive accuracy: {accuracy_score(y_test,y_pred):.3f}\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Random forest predictive accuracy: 0.978\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08o92INxMNQ0",
        "outputId": "7ffcb329-580b-4b13-8536-16e5c6ec140e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Confusion matrix\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "print(matrix)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 72   3]\n",
            " [  1 107]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dN-emBmv8zrv"
      },
      "source": [
        "### ***Logistic Regression Model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyhJm6re8zrv"
      },
      "source": [
        "# Create a Logistic Regression Model\n",
        "classifier = LogisticRegression(solver='lbfgs',\n",
        "                                max_iter=200,\n",
        "                                random_state=1)\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4V5X1CAc8zry",
        "outputId": "6f7ab1c8-91d4-4abf-a043-311be3bfa103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Fit (train) or model using the training data\n",
        "classifier.fit(X_train_scaled, y_train)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=200,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=1, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdxN4orO8zr0",
        "outputId": "625f274c-bedc-412c-9fe1-6fecf8873bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "# Make predictions\n",
        "y_pred = classifier.predict(X_test_scaled)\n",
        "results = pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test}).reset_index(drop=True)\n",
        "results.head(20)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Prediction</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Prediction  Actual\n",
              "0            0       0\n",
              "1            0       0\n",
              "2            1       1\n",
              "3            0       0\n",
              "4            0       0\n",
              "5            1       1\n",
              "6            1       1\n",
              "7            1       1\n",
              "8            1       1\n",
              "9            1       1\n",
              "10           0       0\n",
              "11           0       0\n",
              "12           1       1\n",
              "13           0       0\n",
              "14           0       0\n",
              "15           0       0\n",
              "16           1       0\n",
              "17           1       1\n",
              "18           1       1\n",
              "19           1       1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzHFt7yg8zr1",
        "outputId": "2b26edb6-f1fe-4a08-bf75-740f47c3836d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.907103825136612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QA9-0QZgMR16",
        "outputId": "dfe2c01f-9d24-4acb-c1a8-01d1043b674b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Confusion matrix\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "print(matrix)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[68  7]\n",
            " [10 98]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-KV7kap9ER0"
      },
      "source": [
        "## ***Wine, Weather & Soil***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfFjVq0Z9ER2"
      },
      "source": [
        "# Split our preprocessed data into our features and target arrays\n",
        "y = White_Soil_ML_df[\"quality\"].values\n",
        "X = White_Soil_ML_df.drop([\"quality\",\"is_primeurs\", \"journalist_count\", \"confidence_index_A\",\"confidence_index_A+\", \"confidence_index_B\", \"confidence_index_B+\",\"confidence_index_C\", \"confidence_index_C+\"],1).values"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQIo8cDn9ER5"
      },
      "source": [
        "# Create a StandardScaler instance\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the StandardScaler\n",
        "X_scaler = scaler.fit(X)\n",
        "X_scaled = X_scaler.transform(X)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRhr3mgMMbQM"
      },
      "source": [
        "# Split the preprocessed data into a training and testing dataset\n",
        "X_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, random_state=45)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MxQ4Kv79ER8"
      },
      "source": [
        "###***Deep Learning Neural Netwrok for training data***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z82pNgZC9ER9",
        "outputId": "f113278a-b06b-42f8-b6b7-2831f16bd05d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define the model - deep neural net\n",
        "number_input_features = len(X_train_scaled[-1])\n",
        "hidden_nodes_layer1 =  24\n",
        "hidden_nodes_layer2 = 12\n",
        "\n",
        "# metrics = [\n",
        "#     tf.keras.metrics.FalseNegatives(name=\"fn\"),\n",
        "#     tf.keras.metrics.FalsePositives(name=\"fp\"),\n",
        "#     tf.keras.metrics.TrueNegatives(name=\"tn\"),\n",
        "#     tf.keras.metrics.TruePositives(name=\"tp\"),\n",
        "#     tf.keras.metrics.Precision(name=\"precision\"),\n",
        "#     tf.keras.metrics.Recall(name=\"recall\"),\n",
        "# ]\n",
        "\n",
        "nn = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "\n",
        "# Output layer\n",
        "nn.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Compile the Sequential model together and customize metrics\n",
        "nn.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=False), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Train the model\n",
        "fit_model = nn.fit(X_train_scaled, y_train, epochs=50, validation_split=0.5)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "9/9 [==============================] - 0s 19ms/step - loss: 0.6969 - accuracy: 0.5547 - val_loss: 0.6665 - val_accuracy: 0.6145\n",
            "Epoch 2/50\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.6040 - accuracy: 0.6898 - val_loss: 0.6207 - val_accuracy: 0.6655\n",
            "Epoch 3/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.5454 - accuracy: 0.7555 - val_loss: 0.5880 - val_accuracy: 0.6873\n",
            "Epoch 4/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4997 - accuracy: 0.7847 - val_loss: 0.5623 - val_accuracy: 0.6982\n",
            "Epoch 5/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.4633 - accuracy: 0.8321 - val_loss: 0.5370 - val_accuracy: 0.7345\n",
            "Epoch 6/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4292 - accuracy: 0.8540 - val_loss: 0.5120 - val_accuracy: 0.7527\n",
            "Epoch 7/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3981 - accuracy: 0.8796 - val_loss: 0.4924 - val_accuracy: 0.7564\n",
            "Epoch 8/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3702 - accuracy: 0.8832 - val_loss: 0.4723 - val_accuracy: 0.7818\n",
            "Epoch 9/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3438 - accuracy: 0.8796 - val_loss: 0.4533 - val_accuracy: 0.7927\n",
            "Epoch 10/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3175 - accuracy: 0.8905 - val_loss: 0.4363 - val_accuracy: 0.8000\n",
            "Epoch 11/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2945 - accuracy: 0.9088 - val_loss: 0.4212 - val_accuracy: 0.8109\n",
            "Epoch 12/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.2725 - accuracy: 0.9197 - val_loss: 0.4052 - val_accuracy: 0.8073\n",
            "Epoch 13/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2532 - accuracy: 0.9270 - val_loss: 0.3949 - val_accuracy: 0.8145\n",
            "Epoch 14/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2369 - accuracy: 0.9270 - val_loss: 0.3854 - val_accuracy: 0.8145\n",
            "Epoch 15/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.2206 - accuracy: 0.9380 - val_loss: 0.3813 - val_accuracy: 0.8109\n",
            "Epoch 16/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2067 - accuracy: 0.9380 - val_loss: 0.3713 - val_accuracy: 0.8218\n",
            "Epoch 17/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1931 - accuracy: 0.9416 - val_loss: 0.3662 - val_accuracy: 0.8218\n",
            "Epoch 18/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1819 - accuracy: 0.9416 - val_loss: 0.3570 - val_accuracy: 0.8327\n",
            "Epoch 19/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1706 - accuracy: 0.9453 - val_loss: 0.3481 - val_accuracy: 0.8400\n",
            "Epoch 20/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1601 - accuracy: 0.9562 - val_loss: 0.3478 - val_accuracy: 0.8364\n",
            "Epoch 21/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1505 - accuracy: 0.9599 - val_loss: 0.3428 - val_accuracy: 0.8582\n",
            "Epoch 22/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1421 - accuracy: 0.9599 - val_loss: 0.3432 - val_accuracy: 0.8545\n",
            "Epoch 23/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1350 - accuracy: 0.9635 - val_loss: 0.3388 - val_accuracy: 0.8618\n",
            "Epoch 24/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1269 - accuracy: 0.9635 - val_loss: 0.3314 - val_accuracy: 0.8655\n",
            "Epoch 25/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1208 - accuracy: 0.9599 - val_loss: 0.3301 - val_accuracy: 0.8691\n",
            "Epoch 26/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1143 - accuracy: 0.9635 - val_loss: 0.3273 - val_accuracy: 0.8655\n",
            "Epoch 27/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1085 - accuracy: 0.9635 - val_loss: 0.3275 - val_accuracy: 0.8691\n",
            "Epoch 28/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1042 - accuracy: 0.9708 - val_loss: 0.3283 - val_accuracy: 0.8691\n",
            "Epoch 29/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0990 - accuracy: 0.9708 - val_loss: 0.3279 - val_accuracy: 0.8764\n",
            "Epoch 30/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0942 - accuracy: 0.9708 - val_loss: 0.3230 - val_accuracy: 0.8764\n",
            "Epoch 31/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0893 - accuracy: 0.9818 - val_loss: 0.3257 - val_accuracy: 0.8873\n",
            "Epoch 32/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0864 - accuracy: 0.9781 - val_loss: 0.3198 - val_accuracy: 0.8909\n",
            "Epoch 33/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0818 - accuracy: 0.9854 - val_loss: 0.3214 - val_accuracy: 0.8873\n",
            "Epoch 34/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0785 - accuracy: 0.9854 - val_loss: 0.3230 - val_accuracy: 0.8873\n",
            "Epoch 35/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0758 - accuracy: 0.9854 - val_loss: 0.3239 - val_accuracy: 0.8873\n",
            "Epoch 36/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0733 - accuracy: 0.9891 - val_loss: 0.3174 - val_accuracy: 0.8909\n",
            "Epoch 37/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0720 - accuracy: 0.9891 - val_loss: 0.3117 - val_accuracy: 0.8945\n",
            "Epoch 38/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0701 - accuracy: 0.9927 - val_loss: 0.3298 - val_accuracy: 0.8873\n",
            "Epoch 39/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0653 - accuracy: 0.9964 - val_loss: 0.3220 - val_accuracy: 0.8873\n",
            "Epoch 40/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0629 - accuracy: 0.9927 - val_loss: 0.3145 - val_accuracy: 0.8909\n",
            "Epoch 41/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0609 - accuracy: 0.9891 - val_loss: 0.3174 - val_accuracy: 0.8873\n",
            "Epoch 42/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0602 - accuracy: 0.9927 - val_loss: 0.3214 - val_accuracy: 0.8873\n",
            "Epoch 43/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0556 - accuracy: 0.9964 - val_loss: 0.3147 - val_accuracy: 0.8909\n",
            "Epoch 44/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0557 - accuracy: 0.9927 - val_loss: 0.3127 - val_accuracy: 0.8909\n",
            "Epoch 45/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0534 - accuracy: 0.9927 - val_loss: 0.3205 - val_accuracy: 0.8873\n",
            "Epoch 46/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0511 - accuracy: 0.9964 - val_loss: 0.3171 - val_accuracy: 0.8909\n",
            "Epoch 47/50\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0504 - accuracy: 0.9964 - val_loss: 0.3143 - val_accuracy: 0.8909\n",
            "Epoch 48/50\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0493 - accuracy: 0.9927 - val_loss: 0.3168 - val_accuracy: 0.8982\n",
            "Epoch 49/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0478 - accuracy: 0.9927 - val_loss: 0.3205 - val_accuracy: 0.8945\n",
            "Epoch 50/50\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0449 - accuracy: 0.9964 - val_loss: 0.3186 - val_accuracy: 0.8982\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD_aJFLuMu-l",
        "outputId": "a3484fe4-fbfe-4d6f-e03e-11f893518dbf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "tf.keras.utils.plot_model(nn, show_shapes=True, rankdir=\"LR\")"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkHkpRVZ9ER_"
      },
      "source": [
        "###***Deep Learning Neural Netwrok Evaluation***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcnZuE7rtSTA",
        "outputId": "f3c61998-acbc-4325-a871-d129cb240dd7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Evaluate the model using the test data\n",
        "model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6/6 - 0s - loss: 0.6295 - accuracy: 0.7978\n",
            "Loss: 0.6294720768928528, Accuracy: 0.7978141903877258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SsMVbdKPJqb"
      },
      "source": [
        "**Try with different hidden layer**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a7GuBX7NvmN",
        "outputId": "482d6eb7-d667-45fd-86ca-833ced38c86d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define the model - deep neural net\n",
        "number_input_features = len(X_train_scaled[-1])\n",
        "hidden_nodes_layer1 =  32\n",
        "hidden_nodes_layer2 = 12\n",
        "\n",
        "nn1 = tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn1.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn1.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "# Output layer\n",
        "nn1.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn1.summary()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 32)                6816      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 12)                396       \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 1)                 13        \n",
            "=================================================================\n",
            "Total params: 7,225\n",
            "Trainable params: 7,225\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHbKRTCoNv0l",
        "outputId": "3dd45838-605f-440b-8aa3-98cc76fae581",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nn1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "# Train the model\n",
        "fit_model = nn1.fit(X_train_scaled,y_train,epochs=100,validation_split=0.5)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - 0s 17ms/step - loss: 0.7070 - accuracy: 0.5292 - val_loss: 0.6477 - val_accuracy: 0.6000\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.5894 - accuracy: 0.7044 - val_loss: 0.5654 - val_accuracy: 0.7236\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.5182 - accuracy: 0.7774 - val_loss: 0.5198 - val_accuracy: 0.7673\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4672 - accuracy: 0.8358 - val_loss: 0.4896 - val_accuracy: 0.7709\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.4255 - accuracy: 0.8577 - val_loss: 0.4672 - val_accuracy: 0.7782\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3907 - accuracy: 0.8796 - val_loss: 0.4476 - val_accuracy: 0.8000\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3581 - accuracy: 0.8796 - val_loss: 0.4292 - val_accuracy: 0.8073\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.3311 - accuracy: 0.8869 - val_loss: 0.4113 - val_accuracy: 0.8109\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.3044 - accuracy: 0.9088 - val_loss: 0.3973 - val_accuracy: 0.8109\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2829 - accuracy: 0.9124 - val_loss: 0.3864 - val_accuracy: 0.8109\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2642 - accuracy: 0.9197 - val_loss: 0.3735 - val_accuracy: 0.8364\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2473 - accuracy: 0.9197 - val_loss: 0.3612 - val_accuracy: 0.8364\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.2316 - accuracy: 0.9197 - val_loss: 0.3590 - val_accuracy: 0.8291\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.2160 - accuracy: 0.9234 - val_loss: 0.3511 - val_accuracy: 0.8400\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.2021 - accuracy: 0.9234 - val_loss: 0.3423 - val_accuracy: 0.8473\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.1909 - accuracy: 0.9380 - val_loss: 0.3432 - val_accuracy: 0.8436\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1751 - accuracy: 0.9526 - val_loss: 0.3307 - val_accuracy: 0.8473\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1634 - accuracy: 0.9526 - val_loss: 0.3268 - val_accuracy: 0.8545\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1532 - accuracy: 0.9599 - val_loss: 0.3210 - val_accuracy: 0.8618\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1435 - accuracy: 0.9635 - val_loss: 0.3212 - val_accuracy: 0.8509\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1363 - accuracy: 0.9562 - val_loss: 0.3249 - val_accuracy: 0.8727\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1278 - accuracy: 0.9599 - val_loss: 0.3195 - val_accuracy: 0.8618\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1198 - accuracy: 0.9672 - val_loss: 0.3125 - val_accuracy: 0.8800\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1129 - accuracy: 0.9708 - val_loss: 0.3078 - val_accuracy: 0.8836\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.1074 - accuracy: 0.9708 - val_loss: 0.3169 - val_accuracy: 0.8836\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0999 - accuracy: 0.9708 - val_loss: 0.3032 - val_accuracy: 0.8873\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0961 - accuracy: 0.9781 - val_loss: 0.2999 - val_accuracy: 0.8836\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0911 - accuracy: 0.9818 - val_loss: 0.3148 - val_accuracy: 0.8836\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0848 - accuracy: 0.9891 - val_loss: 0.3024 - val_accuracy: 0.8909\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0804 - accuracy: 0.9891 - val_loss: 0.3011 - val_accuracy: 0.8909\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0804 - accuracy: 0.9927 - val_loss: 0.3127 - val_accuracy: 0.8909\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0709 - accuracy: 0.9927 - val_loss: 0.3024 - val_accuracy: 0.8982\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0689 - accuracy: 0.9891 - val_loss: 0.3057 - val_accuracy: 0.8945\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0640 - accuracy: 0.9927 - val_loss: 0.2995 - val_accuracy: 0.8945\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0618 - accuracy: 0.9964 - val_loss: 0.3005 - val_accuracy: 0.8982\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0575 - accuracy: 0.9964 - val_loss: 0.3121 - val_accuracy: 0.8945\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0572 - accuracy: 0.9964 - val_loss: 0.3137 - val_accuracy: 0.8945\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0520 - accuracy: 0.9964 - val_loss: 0.3021 - val_accuracy: 0.9018\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0505 - accuracy: 0.9964 - val_loss: 0.3038 - val_accuracy: 0.9018\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0493 - accuracy: 0.9964 - val_loss: 0.3133 - val_accuracy: 0.8982\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0465 - accuracy: 0.9964 - val_loss: 0.3115 - val_accuracy: 0.8945\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0445 - accuracy: 0.9964 - val_loss: 0.3058 - val_accuracy: 0.9018\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0413 - accuracy: 0.9964 - val_loss: 0.3122 - val_accuracy: 0.8982\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0390 - accuracy: 0.9964 - val_loss: 0.3133 - val_accuracy: 0.8982\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0383 - accuracy: 0.9964 - val_loss: 0.3134 - val_accuracy: 0.8982\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0373 - accuracy: 0.9964 - val_loss: 0.3180 - val_accuracy: 0.9018\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0350 - accuracy: 0.9964 - val_loss: 0.3170 - val_accuracy: 0.8982\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0339 - accuracy: 0.9964 - val_loss: 0.3176 - val_accuracy: 0.8945\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0337 - accuracy: 0.9964 - val_loss: 0.3172 - val_accuracy: 0.9018\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0314 - accuracy: 0.9964 - val_loss: 0.3144 - val_accuracy: 0.8982\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0308 - accuracy: 0.9964 - val_loss: 0.3197 - val_accuracy: 0.8982\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0285 - accuracy: 0.9964 - val_loss: 0.3259 - val_accuracy: 0.9018\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0272 - accuracy: 0.9964 - val_loss: 0.3310 - val_accuracy: 0.8945\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0269 - accuracy: 0.9964 - val_loss: 0.3197 - val_accuracy: 0.8982\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0257 - accuracy: 0.9964 - val_loss: 0.3275 - val_accuracy: 0.8982\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0249 - accuracy: 0.9964 - val_loss: 0.3278 - val_accuracy: 0.8982\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0239 - accuracy: 0.9964 - val_loss: 0.3290 - val_accuracy: 0.8945\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0234 - accuracy: 0.9964 - val_loss: 0.3291 - val_accuracy: 0.9018\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0222 - accuracy: 0.9964 - val_loss: 0.3281 - val_accuracy: 0.8982\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0210 - accuracy: 0.9964 - val_loss: 0.3332 - val_accuracy: 0.9018\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.3405 - val_accuracy: 0.8982\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0194 - accuracy: 1.0000 - val_loss: 0.3402 - val_accuracy: 0.8982\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0195 - accuracy: 0.9964 - val_loss: 0.3321 - val_accuracy: 0.9018\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0187 - accuracy: 0.9964 - val_loss: 0.3415 - val_accuracy: 0.9018\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.3453 - val_accuracy: 0.8982\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0182 - accuracy: 0.9964 - val_loss: 0.3366 - val_accuracy: 0.9018\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.3484 - val_accuracy: 0.8982\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.3500 - val_accuracy: 0.8982\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0151 - accuracy: 0.9964 - val_loss: 0.3432 - val_accuracy: 0.8982\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0158 - accuracy: 0.9964 - val_loss: 0.3468 - val_accuracy: 0.9018\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.3452 - val_accuracy: 0.8982\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0148 - accuracy: 0.9964 - val_loss: 0.3512 - val_accuracy: 0.9018\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.3552 - val_accuracy: 0.9018\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.3572 - val_accuracy: 0.9018\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.3544 - val_accuracy: 0.9018\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0119 - accuracy: 0.9964 - val_loss: 0.3537 - val_accuracy: 0.9018\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.3593 - val_accuracy: 0.9018\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.3618 - val_accuracy: 0.9018\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0124 - accuracy: 0.9964 - val_loss: 0.3568 - val_accuracy: 0.9018\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0098 - accuracy: 1.0000 - val_loss: 0.3634 - val_accuracy: 0.9018\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.3653 - val_accuracy: 0.9018\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.3593 - val_accuracy: 0.9018\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0096 - accuracy: 1.0000 - val_loss: 0.3677 - val_accuracy: 0.9018\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.3761 - val_accuracy: 0.9018\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - 0s 7ms/step - loss: 0.0091 - accuracy: 1.0000 - val_loss: 0.3675 - val_accuracy: 0.9018\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.3682 - val_accuracy: 0.9018\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0087 - accuracy: 1.0000 - val_loss: 0.3709 - val_accuracy: 0.9018\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3784 - val_accuracy: 0.9018\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0092 - accuracy: 1.0000 - val_loss: 0.3747 - val_accuracy: 0.9018\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.3715 - val_accuracy: 0.9018\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0077 - accuracy: 1.0000 - val_loss: 0.3758 - val_accuracy: 0.9018\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.3799 - val_accuracy: 0.9018\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0074 - accuracy: 1.0000 - val_loss: 0.3814 - val_accuracy: 0.9018\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0076 - accuracy: 1.0000 - val_loss: 0.3815 - val_accuracy: 0.9018\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0085 - accuracy: 0.9964 - val_loss: 0.3757 - val_accuracy: 0.9018\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - 0s 4ms/step - loss: 0.0086 - accuracy: 1.0000 - val_loss: 0.3914 - val_accuracy: 0.9018\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0070 - accuracy: 1.0000 - val_loss: 0.3844 - val_accuracy: 0.9018\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0072 - accuracy: 1.0000 - val_loss: 0.3852 - val_accuracy: 0.9018\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - 0s 5ms/step - loss: 0.0062 - accuracy: 1.0000 - val_loss: 0.3875 - val_accuracy: 0.9018\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - 0s 6ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.3919 - val_accuracy: 0.9018\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PPefozsPNv8K",
        "outputId": "ff1e1596-8af4-48c5-bedf-65f489f55f59",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_loss, model_accuracy = nn1.evaluate(X_train_scaled,y_train,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18/18 - 0s - loss: 0.1992 - accuracy: 0.9508\n",
            "Loss: 0.19924163818359375, Accuracy: 0.9508196711540222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNCmp6IwNv_U",
        "outputId": "998ec8ed-ce50-424f-94cf-b1a865a8148b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Define the model - deep neural net\n",
        "number_input_features = len(X_train_scaled[-1])\n",
        "hidden_nodes_layer1 =  8\n",
        "hidden_nodes_layer2 = 8\n",
        "\n",
        "nn1= tf.keras.models.Sequential()\n",
        "\n",
        "# First hidden layer\n",
        "nn1.add(\n",
        "    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation=\"relu\")\n",
        ")\n",
        "\n",
        "# Second hidden layer\n",
        "nn1.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation=\"relu\"))\n",
        "\n",
        "# Output layer\n",
        "nn1.add(tf.keras.layers.Dense(units=1, activation=\"sigmoid\"))\n",
        "\n",
        "# Check the structure of the model\n",
        "nn1.summary()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_18 (Dense)             (None, 8)                 1704      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 8)                 72        \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 1)                 9         \n",
            "=================================================================\n",
            "Total params: 1,785\n",
            "Trainable params: 1,785\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPU-p3P7CkuV"
      },
      "source": [
        "# Compile the model\n",
        "nn1.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsP3BmPCCo5E",
        "outputId": "26ebd18b-e94e-4a07-ddb6-dd9d07c242f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train the model\n",
        "fit_model = nn1.fit(X_train_scaled,y_train,epochs=100)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.7252 - accuracy: 0.5173\n",
            "Epoch 2/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6667 - accuracy: 0.5756\n",
            "Epoch 3/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.6205 - accuracy: 0.6357\n",
            "Epoch 4/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5835 - accuracy: 0.7122\n",
            "Epoch 5/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5509 - accuracy: 0.7486\n",
            "Epoch 6/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.5230 - accuracy: 0.7705\n",
            "Epoch 7/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4986 - accuracy: 0.7832\n",
            "Epoch 8/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.7996\n",
            "Epoch 9/100\n",
            "18/18 [==============================] - 0s 1ms/step - loss: 0.4501 - accuracy: 0.8051\n",
            "Epoch 10/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.4233 - accuracy: 0.8233\n",
            "Epoch 11/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8361\n",
            "Epoch 12/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3709 - accuracy: 0.8597\n",
            "Epoch 13/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3459 - accuracy: 0.8670\n",
            "Epoch 14/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.3219 - accuracy: 0.8761\n",
            "Epoch 15/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.2999 - accuracy: 0.8889\n",
            "Epoch 16/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.2795 - accuracy: 0.8962\n",
            "Epoch 17/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.2620 - accuracy: 0.9016\n",
            "Epoch 18/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.2428 - accuracy: 0.9107\n",
            "Epoch 19/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.2254 - accuracy: 0.9199\n",
            "Epoch 20/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.2124 - accuracy: 0.9217\n",
            "Epoch 21/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1994 - accuracy: 0.9271\n",
            "Epoch 22/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1876 - accuracy: 0.9381\n",
            "Epoch 23/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1792 - accuracy: 0.9381\n",
            "Epoch 24/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1691 - accuracy: 0.9435\n",
            "Epoch 25/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1606 - accuracy: 0.9435\n",
            "Epoch 26/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1541 - accuracy: 0.9490\n",
            "Epoch 27/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1457 - accuracy: 0.9490\n",
            "Epoch 28/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1378 - accuracy: 0.9472\n",
            "Epoch 29/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1315 - accuracy: 0.9545\n",
            "Epoch 30/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1248 - accuracy: 0.9636\n",
            "Epoch 31/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1239 - accuracy: 0.9563\n",
            "Epoch 32/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1166 - accuracy: 0.9599\n",
            "Epoch 33/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9636\n",
            "Epoch 34/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1059 - accuracy: 0.9690\n",
            "Epoch 35/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.1020 - accuracy: 0.9654\n",
            "Epoch 36/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0998 - accuracy: 0.9672\n",
            "Epoch 37/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0957 - accuracy: 0.9654\n",
            "Epoch 38/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0904 - accuracy: 0.9709\n",
            "Epoch 39/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0867 - accuracy: 0.9763\n",
            "Epoch 40/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0833 - accuracy: 0.9763\n",
            "Epoch 41/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0800 - accuracy: 0.9781\n",
            "Epoch 42/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0772 - accuracy: 0.9800\n",
            "Epoch 43/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0751 - accuracy: 0.9800\n",
            "Epoch 44/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9818\n",
            "Epoch 45/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0724 - accuracy: 0.9836\n",
            "Epoch 46/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0684 - accuracy: 0.9854\n",
            "Epoch 47/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0666 - accuracy: 0.9836\n",
            "Epoch 48/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0657 - accuracy: 0.9818\n",
            "Epoch 49/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0624 - accuracy: 0.9872\n",
            "Epoch 50/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0616 - accuracy: 0.9854\n",
            "Epoch 51/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0601 - accuracy: 0.9854\n",
            "Epoch 52/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0594 - accuracy: 0.9836\n",
            "Epoch 53/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0566 - accuracy: 0.9854\n",
            "Epoch 54/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0554 - accuracy: 0.9854\n",
            "Epoch 55/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0538 - accuracy: 0.9872\n",
            "Epoch 56/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9854\n",
            "Epoch 57/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0517 - accuracy: 0.9854\n",
            "Epoch 58/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0500 - accuracy: 0.9854\n",
            "Epoch 59/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0497 - accuracy: 0.9854\n",
            "Epoch 60/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0488 - accuracy: 0.9854\n",
            "Epoch 61/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0479 - accuracy: 0.9854\n",
            "Epoch 62/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0468 - accuracy: 0.9854\n",
            "Epoch 63/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0459 - accuracy: 0.9854\n",
            "Epoch 64/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0434 - accuracy: 0.9854\n",
            "Epoch 65/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0437 - accuracy: 0.9854\n",
            "Epoch 66/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0423 - accuracy: 0.9854\n",
            "Epoch 67/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0418 - accuracy: 0.9836\n",
            "Epoch 68/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0399 - accuracy: 0.9854\n",
            "Epoch 69/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0383 - accuracy: 0.9872\n",
            "Epoch 70/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0375 - accuracy: 0.9854\n",
            "Epoch 71/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0373 - accuracy: 0.9854\n",
            "Epoch 72/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0361 - accuracy: 0.9872\n",
            "Epoch 73/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0345 - accuracy: 0.9872\n",
            "Epoch 74/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0340 - accuracy: 0.9872\n",
            "Epoch 75/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0335 - accuracy: 0.9872\n",
            "Epoch 76/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0337 - accuracy: 0.9854\n",
            "Epoch 77/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0327 - accuracy: 0.9891\n",
            "Epoch 78/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0322 - accuracy: 0.9836\n",
            "Epoch 79/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0305 - accuracy: 0.9872\n",
            "Epoch 80/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0302 - accuracy: 0.9872\n",
            "Epoch 81/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0295 - accuracy: 0.9872\n",
            "Epoch 82/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.9854\n",
            "Epoch 83/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 0.9891\n",
            "Epoch 84/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0283 - accuracy: 0.9891\n",
            "Epoch 85/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 0.9891\n",
            "Epoch 86/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0255 - accuracy: 0.9891\n",
            "Epoch 87/100\n",
            "18/18 [==============================] - 0s 3ms/step - loss: 0.0245 - accuracy: 0.9872\n",
            "Epoch 88/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0238 - accuracy: 0.9891\n",
            "Epoch 89/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0230 - accuracy: 0.9891\n",
            "Epoch 90/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0246 - accuracy: 0.9927\n",
            "Epoch 91/100\n",
            "18/18 [==============================] - 0s 4ms/step - loss: 0.0254 - accuracy: 0.9927\n",
            "Epoch 92/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0227 - accuracy: 0.9945\n",
            "Epoch 93/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0210 - accuracy: 0.9945\n",
            "Epoch 94/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0222 - accuracy: 0.9945\n",
            "Epoch 95/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0205 - accuracy: 0.9945\n",
            "Epoch 96/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0204 - accuracy: 0.9945\n",
            "Epoch 97/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0214 - accuracy: 0.9945\n",
            "Epoch 98/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0201 - accuracy: 0.9945\n",
            "Epoch 99/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0191 - accuracy: 0.9945\n",
            "Epoch 100/100\n",
            "18/18 [==============================] - 0s 2ms/step - loss: 0.0189 - accuracy: 0.9945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmlYlc1uDACe",
        "outputId": "033f15dd-c86e-49dd-a3d2-f6730efafa9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model_loss, model_accuracy = nn1.evaluate(X_train_scaled,y_train,verbose=2)\n",
        "print(f\"Loss: {model_loss}, Accuracy: {model_accuracy}\")"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18/18 - 0s - loss: 0.0178 - accuracy: 0.9964\n",
            "Loss: 0.017770588397979736, Accuracy: 0.9963570237159729\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}